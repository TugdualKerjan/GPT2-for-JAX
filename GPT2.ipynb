{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Implementing GPT2 in JAX for fun ü¶Äü¶Äü¶Ä\"\n",
    "author:\n",
    "  - name: \"Tugdual Kerjan\"\n",
    "    url: https://tugdual.fr\n",
    "    email: tkerjan@outlook.com\n",
    "date: \"November 9, 2024\"\n",
    "number-sections: true\n",
    "reference-location: margin\n",
    "toc: true\n",
    "format: \n",
    "  html:\n",
    "    standalone: true\n",
    "    embed-resources: true\n",
    "    self-contained-math: true\n",
    "    code-fold: false\n",
    "    code-tools: true\n",
    "execute:\n",
    "  output:\n",
    "    false\n",
    "bibliography: assets/bib.bibtex\n",
    "theme: united\n",
    "github: \"https://github.com/TugdualKerjan/GPT2-for-JAX\"\n",
    "lightbox: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 for JAX üöÄ  \n",
    "\n",
    "Explore the full project on the [GitHub repository](https://github.com/TugdualKerjan/GPT2-for-JAX).\n",
    "\n",
    "## Context ‚úçÔ∏è  \n",
    "\n",
    "This project involves rewriting XTTS in JAX to better understand its architecture and functionality. Originally developed by the now-defunct Coqai company, XTTS is a Text-to-Speech model. We'll recreate its generative component using a GPT2 architecture‚Äîa decoder-only transformer‚Äîbased on [@radford2019language]. The implementation closely follows this [tutorial](https://huggingface.co/blog/sachithgunasekara/nanojaxgpt).  \n",
    "\n",
    "![The crux of the GPT2 architecture. Layers composed of masked attention and forwards.](assets/architecture.png)  \n",
    "\n",
    "\n",
    "## GPT2 in Text-to-Speech  \n",
    "\n",
    "### What are we building?  \n",
    "\n",
    "Our goal is to generate sequences of tokens for audio synthesis. Specifically, we aim to produce \"audio tokens,\" small units of audio, discovered using a [VQVAE](https://tugdual.fr/Audio-VQVAE-for-JAX/). By learning to map text tokens to audio tokens, the model becomes multi-modal.  \n",
    "\n",
    "The final output sequences represent speech, which we convert into audio using [HiFiGAN](https://tugdual.fr/HiFiGAN-for-JAX/). Additionally, we enhance speech expressiveness (e.g., tone, speed) by feeding 1024-dimensional vectors representing the target speaker's paralinguistic features.  \n",
    "\n",
    "### Under the Hood \n",
    "\n",
    "__Masked Attention__ \n",
    "\n",
    "Masked attention is the core mechanism for learning relationships between tokens. It determines which tokens influence others by projecting them into smaller dimensions and computing relationships. Masking ensures the model focuses only on prior tokens, preventing it from \"seeing\" future ones.  \n",
    "\n",
    "Studies classify attention patterns into:  \n",
    "1. **Semantic**: Tokens linked by meaning.  \n",
    "2. **Linguistic**: Tokens connected by grammar (e.g., verbs and nouns).  \n",
    "3. **Rare Tokens**: Infrequent but critical tokens.  \n",
    "\n",
    "__Feedforward Layers__  \n",
    "\n",
    "Feedforward layers mix outputs, add non-linearity via activation functions, and stack layers for hierarchical abstractions. The final output approximates a one-hot encoding in the token vocabulary, enabling token selection for sequential generation.  \n",
    "\n",
    "\n",
    "## Goal üéØ  \n",
    "\n",
    "Implement a GPT2 architecture using Equinox and train it on TinyStories.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We have a few things to implement from the ground up. The custom activation function, the forward layer, the masked attention. We then package this up in a nice layer that we can stack, and finally wrap all these stacks into a GPT2 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by importing our favorite libraries ü•∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "import jax.numpy as jnp\n",
    "import typing as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration file\n",
    "\n",
    "Because of the size of our model, we're going to be passing down lots of arguments. To avoid having a long unreadable list of parameters we can define a \"dataclass\" that will allow us to simply pass a `config` down to the model.\n",
    "\n",
    "Feel free to experiment with various settings !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 200\n",
    "    vocab_size: int = (\n",
    "        50304  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    )\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 512\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = False  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwiGLU Activation Function \n",
    "\n",
    "We start by implementing the SwiGLU activation function, introduced in [@shazeer2020gluvariantsimprovetransformer], a powerful variant of GLU.  \n",
    "\n",
    "### Why SwiGLU?  \n",
    "SwiGLU dynamically adjusts its activation based on the input. Think of it like a railway switch‚Äîredirecting the \"activation path\" when the input carries different information. This gives the network greater flexibility and control, leading to better performance.  \n",
    "\n",
    "For more details, see this [explanation by Boudefel](https://medium.com/@s_boudefel/exploring-swiglu-the-activation-function-powering-modern-llms-9697f88221e7).  \n",
    "\n",
    "![The function we implement, based on [@shazeer2020gluvariantsimprovetransformer]](assets/swiglu.png)  \n",
    "\n",
    "Below is a visualization of the Swish function, $x \\times \\text{sigmoid}(x)$, which plays a role in SwiGLU:  \n",
    "\n",
    "![](assets/graphswi.png)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class SwiGLU(eqx.Module):\n",
    "    W: nn.Linear\n",
    "    V: nn.Linear\n",
    "    b: jax.Array\n",
    "    c: jax.Array\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        self.W = nn.Linear(input_dim, output_dim, key=key1)\n",
    "        self.V = nn.Linear(input_dim, output_dim, key=key2)\n",
    "        self.b = jax.random.normal(key3, (output_dim))\n",
    "        self.c = jax.random.normal(key4, (output_dim))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jax.nn.swish((self.W(x) + self.b) * (self.V(x) + self.c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "key = jax.random.PRNGKey(69)\n",
    "mod = SwiGLU(10, 4, key)\n",
    "\n",
    "x = jnp.ones(10)\n",
    "print(mod(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "We can now move onto the multilayer perceptron, which we mentionned earlier as the feedforward part of our network. Because the model is big and we want to make sure that it doesn't just \"memorize\" things, we include dropout which pushes the model to avoid relying on singular neurons / data flowing through for information.\n",
    "\n",
    "‚ú® You'll also notice that since our SwiGLU has two linear layers in it, in reality each MLP that we'll use uses __4__ layers !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(eqx.Module):\n",
    "    ff1: nn.Linear\n",
    "    ff2: nn.Linear\n",
    "    act: SwiGLU\n",
    "    drop: nn.Dropout\n",
    "\n",
    "    def __init__(self, config, key):\n",
    "\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "        self.ff1 = nn.Linear(\n",
    "            config.n_embd, 4 * config.n_embd, use_bias=config.bias, key=key1\n",
    "        )\n",
    "        self.act = SwiGLU(4 * config.n_embd, 4 * config.n_embd, key=key2)\n",
    "        self.ff2 = nn.Linear(\n",
    "            4 * config.n_embd, config.n_embd, use_bias=config.bias, key=key3\n",
    "        )\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        y = self.ff1(x)\n",
    "        y = self.act(y)\n",
    "        y = self.ff2(y)\n",
    "        return self.drop(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can compare with their implementation to make sure we're close enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "\n",
    "class MLPTheirs(eqx.Module):\n",
    "    c_fc: eqx.nn.Linear\n",
    "    swiglu: SwiGLU\n",
    "    c_proj: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "\n",
    "    def __init__(self, config, key):\n",
    "        lkey1, lkey2, skey = jax.random.split(key, 3)\n",
    "\n",
    "        self.c_fc = eqx.nn.Linear(\n",
    "            config.n_embd, 4 * config.n_embd, use_bias=config.bias, key=lkey1\n",
    "        )\n",
    "        self.swiglu = SwiGLU(4 * config.n_embd, 4 * config.n_embd, skey)\n",
    "        self.c_proj = eqx.nn.Linear(\n",
    "            4 * config.n_embd, config.n_embd, use_bias=config.bias, key=lkey2\n",
    "        )\n",
    "        self.dropout = eqx.nn.Dropout(config.dropout)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = jax.vmap(self.c_fc)(x)\n",
    "        x = jax.vmap(self.swiglu)(x)\n",
    "        x = jax.vmap(self.c_proj)(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "config = GPTConfig()\n",
    "key = jax.random.PRNGKey(69)\n",
    "\n",
    "mlp = MLP(config, key)\n",
    "mlp_theirs = MLPTheirs(config, key)\n",
    "\n",
    "x = jax.random.normal(key, (100, config.n_embd))\n",
    "\n",
    "res = jax.vmap(mlp)(x)\n",
    "res_theirs = mlp_theirs(x)\n",
    "\n",
    "average_diff = jnp.mean(res_theirs)\n",
    "print(average_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked attention\n",
    "\n",
    "Moving onto one of the more complicated aspects of the model, but in the end it simply learns to output which tokens are more important with each other. There are plenty of fantastic tutorials out there for better understanding the underlying concept, notably : [Transformers explained visually](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class CausalSelfAttention(eqx.Module):\n",
    "    attnk: nn.Linear\n",
    "    attnq: nn.Linear\n",
    "    attnv: nn.Linear\n",
    "    proj: nn.Linear\n",
    "\n",
    "    resid_dropout: nn.Dropout\n",
    "    attn_dropout: nn.Dropout\n",
    "\n",
    "    mask: jax.Array\n",
    "\n",
    "    def __init__(self, config, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "\n",
    "        self.attnk = nn.Linear(\n",
    "            config.n_embd, config.n_embd, use_bias=config.bias, key=key1\n",
    "        )\n",
    "        self.attnv = nn.Linear(\n",
    "            config.n_embd, config.n_embd, use_bias=config.bias, key=key2\n",
    "        )\n",
    "        self.attnq = nn.Linear(\n",
    "            config.n_embd, config.n_embd, use_bias=config.bias, key=key3\n",
    "        )\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.proj = nn.Linear(\n",
    "            config.n_embd, config.n_embd, use_bias=config.bias, key=key4\n",
    "        )\n",
    "\n",
    "        self.mask = jnp.tril(jnp.ones((config.block_size, config.block_size)))\n",
    "\n",
    "    # Could play arround with the different attention score calculations (Baidhu ?)\n",
    "    # X is an embedding, it should self attend.\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        # x = jnp.swapaxes(x, -1, -2)\n",
    "        T, C = x.shape  # Seq length and embedding dim.\n",
    "\n",
    "        q = jax.vmap(self.attnq)(x)\n",
    "        k = jax.vmap(self.attnk)(x)\n",
    "        v = jax.vmap(self.attnv)(x)\n",
    "\n",
    "        att = jnp.matmul(q, jnp.transpose(k)) / math.sqrt(jnp.shape(k)[-1])\n",
    "        att = jnp.where(\n",
    "            jax.numpy.equal(jax.lax.stop_gradient(self.mask[:T, :T]), 0),\n",
    "            float(\"-inf\"),\n",
    "            att,\n",
    "        )\n",
    "        att = jax.nn.softmax(att, axis=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "\n",
    "        y = jnp.matmul(att, v)\n",
    "\n",
    "        y = jax.vmap(self.proj)(y)\n",
    "        y = self.resid_dropout(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "import optax\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "key = jax.random.PRNGKey(69)\n",
    "\n",
    "mlp = CausalSelfAttention(config, key)\n",
    "optimizer = optax.adam(1e-5)\n",
    "opt_state = optimizer.init(mlp)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(2), (30, config.n_embd))\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss(model, x, y):\n",
    "    output = model(x)\n",
    "    return jax.numpy.mean(jax.numpy.abs(y - output))\n",
    "\n",
    "\n",
    "def make_step(model, opt_state, x, y):\n",
    "    loss_step, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "    updates, optimizer_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, optimizer_state, loss_step\n",
    "\n",
    "\n",
    "print(make_step(mlp, opt_state, x, x))\n",
    "\n",
    "print(mlp(jax.random.normal(key, (100, config.n_embd))).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block\n",
    "\n",
    "Ok ! Now that we have the component parts of what we call a \"block\" we can assemble them. This will then be stacked to get as many layers of abstraction as we wish. In our case we will stack it 12 times as per the GPTConfig we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class Block(eqx.Module):\n",
    "    norm: nn.LayerNorm\n",
    "    attn: CausalSelfAttention\n",
    "    mlp: MLP\n",
    "\n",
    "    def __init__(self, config, key):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "\n",
    "        self.norm = nn.LayerNorm(config.n_embd, use_bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config, key=key1)\n",
    "        self.mlp = MLP(config, key=key2)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x):\n",
    "        y = jax.vmap(self.norm)(x)\n",
    "        y = self.attn(\n",
    "            y\n",
    "        )  # Can't vmap as the whole point is exchange info between tokens.\n",
    "        x = y + x\n",
    "\n",
    "        y = jax.vmap(self.norm)(x)\n",
    "        y = jax.vmap(self.mlp)(y)\n",
    "        x = y + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can compare with their work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "import optax\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "key = jax.random.PRNGKey(69)\n",
    "\n",
    "block = Block(config, key)\n",
    "optimizer = optax.adam(1e-5)\n",
    "opt_state = optimizer.init(block)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(2), (30, config.n_embd))\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss(model, x, y):\n",
    "    output = model(x)\n",
    "    return jax.numpy.mean(jax.numpy.abs(y - output))\n",
    "\n",
    "\n",
    "def make_step(model, opt_state, x, y):\n",
    "    loss_step, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "    updates, optimizer_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, optimizer_state, loss_step\n",
    "\n",
    "\n",
    "print(make_step(block, opt_state, x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally add the embeddings to our model, which are the maps that send tokens to the dimension that the model works with, i.e. 1024 dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class GPT(eqx.Module):\n",
    "    wte: nn.Embedding  # Token embeddings\n",
    "    wpe: nn.Embedding  # Positional embeddings\n",
    "\n",
    "    drop: nn.Dropout\n",
    "\n",
    "    layers: list\n",
    "    norm: nn.LayerNorm\n",
    "    lm_head: nn.Linear\n",
    "\n",
    "    def __init__(self, config, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd, key=key1)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd, key=key2)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.layers = [Block(config, key3) for _ in range(config.n_layer)]\n",
    "        self.norm = nn.LayerNorm(config.n_embd, use_bias=config.bias)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, key=key4)\n",
    "\n",
    "    # @eqx.filter_jit\n",
    "    def __call__(self, token_ids):\n",
    "        (t,) = token_ids.shape\n",
    "\n",
    "        # Should use better positional embeddings with cos and sin.\n",
    "        pos = jnp.arange(0, t, dtype=jnp.int64)\n",
    "\n",
    "        tok_emb = jax.vmap(self.wte)(token_ids)\n",
    "        pos_emb = jax.vmap(self.wpe)(pos)\n",
    "\n",
    "        # Dropout at the first layer ? Seems a bit aggressive...\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "\n",
    "        for block in self.layers:\n",
    "            x = block(x)\n",
    "        x = jax.vmap(self.norm)(x)\n",
    "        logits = jax.vmap(self.lm_head)(x)\n",
    "        logits = jax.nn.softmax(logits)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPT(\n",
      "  wte=Embedding(num_embeddings=50304, embedding_size=200, weight=f32[50304,200]),\n",
      "  wpe=Embedding(num_embeddings=128, embedding_size=200, weight=f32[128,200]),\n",
      "  drop=Dropout(p=0.0, inference=False),\n",
      "  layers=[\n",
      "    Block(\n",
      "      norm=LayerNorm(\n",
      "        shape=(200,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=False,\n",
      "        weight=f32[200],\n",
      "        bias=None\n",
      "      ),\n",
      "      attn=CausalSelfAttention(\n",
      "        attnk=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnq=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnv=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        proj=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        resid_dropout=Dropout(p=0.0, inference=False),\n",
      "        attn_dropout=Dropout(p=0.0, inference=False),\n",
      "        mask=f32[128,128]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        ff1=Linear(\n",
      "          weight=f32[800,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=800,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        ff2=Linear(\n",
      "          weight=f32[200,800],\n",
      "          bias=None,\n",
      "          in_features=800,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        act=SwiGLU(\n",
      "          W=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          V=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          b=f32[800],\n",
      "          c=f32[800]\n",
      "        ),\n",
      "        drop=Dropout(p=0.0, inference=False)\n",
      "      )\n",
      "    ),\n",
      "    Block(\n",
      "      norm=LayerNorm(\n",
      "        shape=(200,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=False,\n",
      "        weight=f32[200],\n",
      "        bias=None\n",
      "      ),\n",
      "      attn=CausalSelfAttention(\n",
      "        attnk=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnq=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnv=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        proj=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        resid_dropout=Dropout(p=0.0, inference=False),\n",
      "        attn_dropout=Dropout(p=0.0, inference=False),\n",
      "        mask=f32[128,128]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        ff1=Linear(\n",
      "          weight=f32[800,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=800,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        ff2=Linear(\n",
      "          weight=f32[200,800],\n",
      "          bias=None,\n",
      "          in_features=800,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        act=SwiGLU(\n",
      "          W=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          V=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          b=f32[800],\n",
      "          c=f32[800]\n",
      "        ),\n",
      "        drop=Dropout(p=0.0, inference=False)\n",
      "      )\n",
      "    ),\n",
      "    Block(\n",
      "      norm=LayerNorm(\n",
      "        shape=(200,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=False,\n",
      "        weight=f32[200],\n",
      "        bias=None\n",
      "      ),\n",
      "      attn=CausalSelfAttention(\n",
      "        attnk=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnq=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnv=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        proj=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        resid_dropout=Dropout(p=0.0, inference=False),\n",
      "        attn_dropout=Dropout(p=0.0, inference=False),\n",
      "        mask=f32[128,128]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        ff1=Linear(\n",
      "          weight=f32[800,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=800,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        ff2=Linear(\n",
      "          weight=f32[200,800],\n",
      "          bias=None,\n",
      "          in_features=800,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        act=SwiGLU(\n",
      "          W=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          V=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          b=f32[800],\n",
      "          c=f32[800]\n",
      "        ),\n",
      "        drop=Dropout(p=0.0, inference=False)\n",
      "      )\n",
      "    )\n",
      "  ],\n",
      "  norm=LayerNorm(\n",
      "    shape=(200,),\n",
      "    eps=1e-05,\n",
      "    use_weight=True,\n",
      "    use_bias=False,\n",
      "    weight=f32[200],\n",
      "    bias=None\n",
      "  ),\n",
      "  lm_head=Linear(\n",
      "    weight=f32[50304,200],\n",
      "    bias=f32[50304],\n",
      "    in_features=200,\n",
      "    out_features=50304,\n",
      "    use_bias=True\n",
      "  )\n",
      "), (ScaleByAdamState(count=Array(1, dtype=int32), mu=GPT(\n",
      "  wte=Embedding(num_embeddings=50304, embedding_size=200, weight=f32[50304,200]),\n",
      "  wpe=Embedding(num_embeddings=128, embedding_size=200, weight=f32[128,200]),\n",
      "  drop=Dropout(p=None, inference=None),\n",
      "  layers=[\n",
      "    Block(\n",
      "      norm=LayerNorm(\n",
      "        shape=(200,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=False,\n",
      "        weight=f32[200],\n",
      "        bias=None\n",
      "      ),\n",
      "      attn=CausalSelfAttention(\n",
      "        attnk=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnq=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnv=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        proj=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        resid_dropout=Dropout(p=None, inference=None),\n",
      "        attn_dropout=Dropout(p=None, inference=None),\n",
      "        mask=f32[128,128]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        ff1=Linear(\n",
      "          weight=f32[800,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=800,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        ff2=Linear(\n",
      "          weight=f32[200,800],\n",
      "          bias=None,\n",
      "          in_features=800,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        act=SwiGLU(\n",
      "          W=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          V=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          b=f32[800],\n",
      "          c=f32[800]\n",
      "        ),\n",
      "        drop=Dropout(p=None, inference=None)\n",
      "      )\n",
      "    ),\n",
      "    Block(\n",
      "      norm=LayerNorm(\n",
      "        shape=(200,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=False,\n",
      "        weight=f32[200],\n",
      "        bias=None\n",
      "      ),\n",
      "      attn=CausalSelfAttention(\n",
      "        attnk=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnq=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnv=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        proj=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        resid_dropout=Dropout(p=None, inference=None),\n",
      "        attn_dropout=Dropout(p=None, inference=None),\n",
      "        mask=f32[128,128]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        ff1=Linear(\n",
      "          weight=f32[800,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=800,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        ff2=Linear(\n",
      "          weight=f32[200,800],\n",
      "          bias=None,\n",
      "          in_features=800,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        act=SwiGLU(\n",
      "          W=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          V=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          b=f32[800],\n",
      "          c=f32[800]\n",
      "        ),\n",
      "        drop=Dropout(p=None, inference=None)\n",
      "      )\n",
      "    ),\n",
      "    Block(\n",
      "      norm=LayerNorm(\n",
      "        shape=(200,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=False,\n",
      "        weight=f32[200],\n",
      "        bias=None\n",
      "      ),\n",
      "      attn=CausalSelfAttention(\n",
      "        attnk=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnq=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnv=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        proj=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        resid_dropout=Dropout(p=None, inference=None),\n",
      "        attn_dropout=Dropout(p=None, inference=None),\n",
      "        mask=f32[128,128]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        ff1=Linear(\n",
      "          weight=f32[800,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=800,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        ff2=Linear(\n",
      "          weight=f32[200,800],\n",
      "          bias=None,\n",
      "          in_features=800,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        act=SwiGLU(\n",
      "          W=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          V=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          b=f32[800],\n",
      "          c=f32[800]\n",
      "        ),\n",
      "        drop=Dropout(p=None, inference=None)\n",
      "      )\n",
      "    )\n",
      "  ],\n",
      "  norm=LayerNorm(\n",
      "    shape=(200,),\n",
      "    eps=1e-05,\n",
      "    use_weight=True,\n",
      "    use_bias=False,\n",
      "    weight=f32[200],\n",
      "    bias=None\n",
      "  ),\n",
      "  lm_head=Linear(\n",
      "    weight=f32[50304,200],\n",
      "    bias=f32[50304],\n",
      "    in_features=200,\n",
      "    out_features=50304,\n",
      "    use_bias=True\n",
      "  )\n",
      "), nu=GPT(\n",
      "  wte=Embedding(num_embeddings=50304, embedding_size=200, weight=f32[50304,200]),\n",
      "  wpe=Embedding(num_embeddings=128, embedding_size=200, weight=f32[128,200]),\n",
      "  drop=Dropout(p=None, inference=None),\n",
      "  layers=[\n",
      "    Block(\n",
      "      norm=LayerNorm(\n",
      "        shape=(200,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=False,\n",
      "        weight=f32[200],\n",
      "        bias=None\n",
      "      ),\n",
      "      attn=CausalSelfAttention(\n",
      "        attnk=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnq=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnv=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        proj=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        resid_dropout=Dropout(p=None, inference=None),\n",
      "        attn_dropout=Dropout(p=None, inference=None),\n",
      "        mask=f32[128,128]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        ff1=Linear(\n",
      "          weight=f32[800,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=800,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        ff2=Linear(\n",
      "          weight=f32[200,800],\n",
      "          bias=None,\n",
      "          in_features=800,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        act=SwiGLU(\n",
      "          W=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          V=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          b=f32[800],\n",
      "          c=f32[800]\n",
      "        ),\n",
      "        drop=Dropout(p=None, inference=None)\n",
      "      )\n",
      "    ),\n",
      "    Block(\n",
      "      norm=LayerNorm(\n",
      "        shape=(200,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=False,\n",
      "        weight=f32[200],\n",
      "        bias=None\n",
      "      ),\n",
      "      attn=CausalSelfAttention(\n",
      "        attnk=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnq=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnv=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        proj=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        resid_dropout=Dropout(p=None, inference=None),\n",
      "        attn_dropout=Dropout(p=None, inference=None),\n",
      "        mask=f32[128,128]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        ff1=Linear(\n",
      "          weight=f32[800,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=800,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        ff2=Linear(\n",
      "          weight=f32[200,800],\n",
      "          bias=None,\n",
      "          in_features=800,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        act=SwiGLU(\n",
      "          W=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          V=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          b=f32[800],\n",
      "          c=f32[800]\n",
      "        ),\n",
      "        drop=Dropout(p=None, inference=None)\n",
      "      )\n",
      "    ),\n",
      "    Block(\n",
      "      norm=LayerNorm(\n",
      "        shape=(200,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=False,\n",
      "        weight=f32[200],\n",
      "        bias=None\n",
      "      ),\n",
      "      attn=CausalSelfAttention(\n",
      "        attnk=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnq=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        attnv=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        proj=Linear(\n",
      "          weight=f32[200,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        resid_dropout=Dropout(p=None, inference=None),\n",
      "        attn_dropout=Dropout(p=None, inference=None),\n",
      "        mask=f32[128,128]\n",
      "      ),\n",
      "      mlp=MLP(\n",
      "        ff1=Linear(\n",
      "          weight=f32[800,200],\n",
      "          bias=None,\n",
      "          in_features=200,\n",
      "          out_features=800,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        ff2=Linear(\n",
      "          weight=f32[200,800],\n",
      "          bias=None,\n",
      "          in_features=800,\n",
      "          out_features=200,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        act=SwiGLU(\n",
      "          W=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          V=Linear(\n",
      "            weight=f32[800,800],\n",
      "            bias=f32[800],\n",
      "            in_features=800,\n",
      "            out_features=800,\n",
      "            use_bias=True\n",
      "          ),\n",
      "          b=f32[800],\n",
      "          c=f32[800]\n",
      "        ),\n",
      "        drop=Dropout(p=None, inference=None)\n",
      "      )\n",
      "    )\n",
      "  ],\n",
      "  norm=LayerNorm(\n",
      "    shape=(200,),\n",
      "    eps=1e-05,\n",
      "    use_weight=True,\n",
      "    use_bias=False,\n",
      "    weight=f32[200],\n",
      "    bias=None\n",
      "  ),\n",
      "  lm_head=Linear(\n",
      "    weight=f32[50304,200],\n",
      "    bias=f32[50304],\n",
      "    in_features=200,\n",
      "    out_features=50304,\n",
      "    use_bias=True\n",
      "  )\n",
      ")), EmptyState()), Array(10.825845, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "import optax\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "key = jax.random.PRNGKey(69)\n",
    "\n",
    "block = GPT(config, key)\n",
    "optimizer = optax.adam(1e-5)\n",
    "opt_state = optimizer.init(block)\n",
    "\n",
    "x = jax.numpy.ones((30, 128), dtype=jax.numpy.int32)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss(model, x, y):\n",
    "    output = jax.vmap(model)(x)\n",
    "    return jax.numpy.mean(\n",
    "        jax.vmap(optax.softmax_cross_entropy_with_integer_labels)(output, y)\n",
    "    )\n",
    "\n",
    "\n",
    "def make_step(model, opt_state, x, y):\n",
    "    loss_step, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "    updates, optimizer_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, optimizer_state, loss_step\n",
    "\n",
    "\n",
    "print(make_step(block, opt_state, x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move onto training the model ! We're going to be using the TinyStories dataset. [Tiktoken](https://github.com/openai/tiktoken) is used to map the sentences to sequences of tokens that the model would understand. Below is the code to download and transform the data into a binary file, and then provide it with a dataloader to our training regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "# saves the openwebtext dataset to a binary file for training. following was helpful:\n",
    "# https://github.com/HazyResearch/flash-attention/blob/main/training/src/datamodules/language_modeling_hf.py\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset  # huggingface datasets\n",
    "\n",
    "# number of workers in .map() call\n",
    "# good number to use is ~order number of cpu cores // 2\n",
    "num_proc = 16\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# we now want to tokenize the dataset. first define the encoding function (gpt2 bpe)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(\n",
    "        example[\"text\"]\n",
    "    )  # encode_ordinary ignores any special tokens\n",
    "    ids.append(enc.eot_token)  # add the end of text token, e.g. 50256 for gpt2 bpe\n",
    "    # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n",
    "    out = {\"ids\": ids, \"len\": len(ids)}\n",
    "    return out\n",
    "\n",
    "\n",
    "# tokenize the dataset\n",
    "tokenized = dataset.map(\n",
    "    process,\n",
    "    remove_columns=[\"text\"],\n",
    "    desc=\"tokenizing the splits\",\n",
    "    num_proc=num_proc,\n",
    ")\n",
    "\n",
    "# concatenate all the ids in each dataset into one large file we can use for training\n",
    "for split, dset in tokenized.items():\n",
    "    arr_len = np.sum(dset[\"len\"])\n",
    "    filename = os.path.join(os.path.dirname(\"dataset\"), f\"{split}.bin\")\n",
    "    dtype = np.uint16  # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "    arr = np.memmap(filename, dtype=dtype, mode=\"w+\", shape=(arr_len,))\n",
    "    total_batches = 1024\n",
    "\n",
    "    idx = 0\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f\"writing {filename}\"):\n",
    "        # Batch together samples for faster write\n",
    "        batch = dset.shard(\n",
    "            num_shards=total_batches, index=batch_idx, contiguous=True\n",
    "        ).with_format(\"numpy\")\n",
    "        arr_batch = np.concatenate(batch[\"ids\"])\n",
    "        # Write into mmap\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "    arr.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the code from the compressed binary representation to the inputs and outputs. Since we want the GPT to learn to predict the next token, we simply shift the input by 1 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "import os\n",
    "import jax.numpy as np\n",
    "from GPT2 import GPTConfig\n",
    "import numpy\n",
    "\n",
    "data_dir = \"dataset\"\n",
    "config = GPTConfig()\n",
    "\n",
    "\n",
    "def get_batch(split: str):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == \"train\":\n",
    "        data = numpy.memmap(\n",
    "            os.path.join(data_dir, \"train.bin\"), dtype=numpy.uint16, mode=\"r\"\n",
    "        )\n",
    "    else:\n",
    "        data = numpy.memmap(\n",
    "            os.path.join(data_dir, \"validation.bin\"), dtype=numpy.uint16, mode=\"r\"\n",
    "        )\n",
    "\n",
    "    ix = numpy.random.randint(len(data) - config.block_size, size=(8,))\n",
    "    x = np.stack(\n",
    "        [np.array(data[i : i + config.block_size], dtype=np.int64) for i in ix]\n",
    "    )\n",
    "    y = np.stack(\n",
    "        [np.array(data[i + 1 : i + 1 + config.block_size], dtype=np.int64) for i in ix]\n",
    "    )\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our loss function. Our goal here is to motivate the model to output something close to [0, 0, 0, ..., 1,..., 0, 0] where the 1 is placed at the $n$th index. This index would ideally correspond to the word we're attempting to match. `optax`, the ML optimisation library of JAX conveniently has a function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "learning_rate = 1e-5\n",
    "warmup_iters = 10\n",
    "init_from = \"scratch\"\n",
    "lr_decay_iters = 20\n",
    "iter_num = 0\n",
    "min_lr = 1e-6\n",
    "\n",
    "lr_scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=learning_rate,\n",
    "    warmup_steps=warmup_iters if init_from == \"scratch\" else 0,\n",
    "    decay_steps=lr_decay_iters - iter_num,\n",
    "    end_value=min_lr,\n",
    ")\n",
    "\n",
    "optimizer = optax.inject_hyperparams(optax.adamw)(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss(model, x, y):\n",
    "    output = jax.vmap(model)(x)\n",
    "    return jax.numpy.mean(\n",
    "        jax.vmap(optax.softmax_cross_entropy_with_integer_labels)(output, y)\n",
    "    )\n",
    "\n",
    "\n",
    "def make_step(model, optimizer_state, x, y):\n",
    "    losses, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "    updates, optimizer_state = optimizer.update(grads, optimizer_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, optimizer_state, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move onto initializing our model and training it ! We can log the progress on wandb to see the loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "[[ 7967    88    13 ...   645   780   673]\n",
      " [ 1375 15342   683 ...   477   465  1204]\n",
      " [  290   517    13 ...   760    11   475]\n",
      " ...\n",
      " [  339  1625   284 ... 19751    30   520]\n",
      " [   11   339  1965 ...   640  1978    13]\n",
      " [ 6403   373   523 ...  5059  3371   683]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/hdfw6x594jbbkfl6hzwf8cxw0000gn/T/ipykernel_24843/954736718.py:24: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  [np.array(data[i : i + config.block_size], dtype=np.int64) for i in ix]\n",
      "/var/folders/79/hdfw6x594jbbkfl6hzwf8cxw0000gn/T/ipykernel_24843/954736718.py:27: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  [np.array(data[i + 1 : i + 1 + config.block_size], dtype=np.int64) for i in ix]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 128)\n",
      "Iteration 1/100 | Loss: 10.825838088989258\n",
      "[[  467    13   632 ...    13  1649   262]\n",
      " [  287   465  1263 ...  3574   326  1110]\n",
      " [  287   465 13008 ...   607    13  1375]\n",
      " ...\n",
      " [ 2300   703  1327 ...  9461    11  1757]\n",
      " [  290   531    11 ...   957   475  2147]\n",
      " [  714  1100    13 ...    11   484  2982]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/hdfw6x594jbbkfl6hzwf8cxw0000gn/T/ipykernel_24843/954736718.py:24: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  [np.array(data[i : i + config.block_size], dtype=np.int64) for i in ix]\n",
      "/var/folders/79/hdfw6x594jbbkfl6hzwf8cxw0000gn/T/ipykernel_24843/954736718.py:27: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  [np.array(data[i + 1 : i + 1 + config.block_size], dtype=np.int64) for i in ix]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 128)\n",
      "Iteration 2/100 | Loss: 10.825838088989258\n",
      "[[   11  6184    95 ...   523   881  7838]\n",
      " [  339   714   407 ...   355   484   714]\n",
      " [   13   314  1842 ...   284   262  3952]\n",
      " ...\n",
      " [  257  1263  3704 ...  3521   470   651]\n",
      " [  373   257  1310 ...  5822 22075   465]\n",
      " [  465  5318   326 ...   523   326   314]]\n",
      "(8, 128)\n",
      "Iteration 3/100 | Loss: 10.825836181640625\n",
      "[[ 1022   262   734 ...   339  4251   262]\n",
      " [  257  1657    13 ...  9955    11   257]\n",
      " [  351   465  1021 ...   460  1833  4186]\n",
      " ...\n",
      " [  475   339  1422 ...   366 10449   345]\n",
      " [ 6253    13   383 ...   550   284   466]\n",
      " [  736  1363    11 ...   262 16365   644]]\n",
      "(8, 128)\n",
      "Iteration 4/100 | Loss: 10.82583999633789\n",
      "[[  262  2119   351 ...   603   338 10955]\n",
      " [ 1243   326   547 ...   198   198 22940]\n",
      " [  546   284   923 ...  1339   262 11376]\n",
      " ...\n",
      " [  607   290 26834 ...  1110    11   257]\n",
      " [ 1123   584    13 ...   340    13   366]\n",
      " [  711   517 19780 ... 45230   284   262]]\n",
      "(8, 128)\n",
      "Iteration 5/100 | Loss: 10.825838088989258\n",
      "[[  284   307   262 ...   886   290  3066]\n",
      " [   11   339  3066 ...   428 32263    30]\n",
      " [  290 41130  5045 ...   925   477   416]\n",
      " ...\n",
      " [ 7454  2402   257 ...   475   351 12123]\n",
      " [  262  7588  5391 ...    13 50256  7454]\n",
      " [25596   948  1816 ...  4499   326  7373]]\n",
      "(8, 128)\n",
      "Iteration 6/100 | Loss: 10.825837135314941\n",
      "[[   11  5901   351 ...  7454  2402   257]\n",
      " [  644   284   466 ...  1110    11   607]\n",
      " [  326  1701 10609 ...  2156   714   766]\n",
      " ...\n",
      " [  262 10481   290 ...   290   612   547]\n",
      " [  465  1021    13 ...   547  3772    13]\n",
      " [ 1965   683    11 ...  1309   338   467]]\n",
      "(8, 128)\n",
      "Iteration 7/100 | Loss: 10.825838088989258\n",
      "[[  373  7819     0 ... 29252    11   644]\n",
      " [ 1257 48342   351 ...   632  2492   470]\n",
      " [ 5137   340   736 ...  2111   284  3151]\n",
      " ...\n",
      " [ 2342   503   329 ...   366  2504   338]\n",
      " [  284  4186    13 ...   290 19533   290]\n",
      " [ 6639   393  5938 ...    13   198   198]]\n",
      "(8, 128)\n",
      "Iteration 8/100 | Loss: 10.825838088989258\n",
      "[[  788   356   460 ...   460   356   787]\n",
      " [14958   326   547 ...  2460   508  8288]\n",
      " [  257  1310  2576 ...  5201    11   673]\n",
      " ...\n",
      " [  383  4171 16853 ...    13  1119  4601]\n",
      " [  284  4727  1521 ...   645  2392  6776]\n",
      " [  407  2041    13 ... 20037  1139    11]]\n",
      "(8, 128)\n",
      "Iteration 9/100 | Loss: 10.825838088989258\n",
      "[[   13  4186   750 ...   467   284   262]\n",
      " [ 4539  1497    13 ...    13 50256    43]\n",
      " [  198   464  4252 ...   198   198  7554]\n",
      " ...\n",
      " [   13  1119   973 ...    40   716   281]\n",
      " [  290  3772    13 ...  1995   553 24799]\n",
      " [  640   284  1100 ... 42835   290   665]]\n",
      "(8, 128)\n",
      "Iteration 10/100 | Loss: 10.825838088989258\n",
      "[[ 2613  1165  1327 ...   345   460  1394]\n",
      " [  673  1392  2626 ...  2497   262 12833]\n",
      " [ 2061   318   428 ...  2119    13   198]\n",
      " ...\n",
      " [   13  1375  4539 ...   345  1701   198]\n",
      " [20037    13   314 ...   366  5756   338]\n",
      " [  607 38367   531 ...  2279   373  5201]]\n",
      "(8, 128)\n",
      "Iteration 11/100 | Loss: 10.825838088989258\n",
      "[[50256  7454  2402 ...   679   714   766]\n",
      " [  531    11   366 ...   550  9373   262]\n",
      " [ 7069   290  7069 ... 14464    13  1119]\n",
      " ...\n",
      " [  339   550  5071 ... 22603   494  1718]\n",
      " [   13   509 12627 ...   625   262  1627]\n",
      " [  526   198   198 ...   582    13   679]]\n",
      "(8, 128)\n",
      "Iteration 12/100 | Loss: 10.825838088989258\n",
      "[[  257  1263 11376 ... 49890  6949    13]\n",
      " [19751    13   679 ...   257  2089  2128]\n",
      " [   13   198   198 ...   257  1310  2576]\n",
      " ...\n",
      " [ 1468    11   475 ...  1576   284  3151]\n",
      " [   13   198  3198 ...   262  3952    13]\n",
      " [  607     0   198 ...   523  6507   290]]\n",
      "(8, 128)\n",
      "Iteration 13/100 | Loss: 10.825836181640625\n",
      "[[ 2677    11   523 ...    13   679 26280]\n",
      " [10481    13  1375 ...   286  1440    13]\n",
      " [   11  8832   714 ...    11   673  2982]\n",
      " ...\n",
      " [  262  3623  4429 ...    11   612   373]\n",
      " [ 6502   910    13 ...   423   257  1263]\n",
      " [20037  2993   339 ... 20037  2497   607]]\n",
      "(8, 128)\n",
      "Iteration 14/100 | Loss: 10.825838088989258\n",
      "[[ 3142   284  1011 ...   679  1234   262]\n",
      " [ 1119   460   991 ...   198     1  1639]\n",
      " [  611   339   373 ...  6090   314  1037]\n",
      " ...\n",
      " [18177 15063   511 ...   284   262 45128]\n",
      " [ 1645   287   262 ... 10153  1158   290]\n",
      " [  607  1021   656 ...  6807    11  5335]]\n",
      "(8, 128)\n",
      "Iteration 15/100 | Loss: 10.825836181640625\n",
      "[[  428  9015   318 ... 26626   351  6041]\n",
      " [ 1612   706   477 ...    13  1375  8288]\n",
      " [   13  1375 48024 ...  2832    13   921]\n",
      " ...\n",
      " [  673  2497    13 ... 26728   618   673]\n",
      " [  612   373   257 ...   262  7786 33677]\n",
      " [  373   290  4193 ...    13  1375  3111]]\n",
      "(8, 128)\n",
      "Iteration 16/100 | Loss: 10.825837135314941\n",
      "[[  340   481   787 ...   319   257 40529]\n",
      " [  531    13   198 ...   287   606    13]\n",
      " [13279   511  6665 ...    13 11735   290]\n",
      " ...\n",
      " [  588   284   711 ...   198  2990  4574]\n",
      " [   11   356   460 ... 14464   290   531]\n",
      " [ 8347    13 11075 ...   523   881  5876]]\n",
      "(8, 128)\n",
      "Iteration 17/100 | Loss: 10.825836181640625\n",
      "[[  198 31160   290 ...   257  1310  2576]\n",
      " [  281  1593 11483 ...   470   588 44653]\n",
      " [ 3952    13 20037 ...    40   765   284]\n",
      " ...\n",
      " [ 2187  1110    13 ...   651 10032   290]\n",
      " [  284   467  2354 ...   290  1719   257]\n",
      " [ 7454  2402   257 ...  4438   284  2822]]\n",
      "(8, 128)\n",
      "Iteration 18/100 | Loss: 10.825836181640625\n",
      "[[  470   423  1576 ...  1683  2058   422]\n",
      " [  329   534  3956 ... 16225  1123   584]\n",
      " [  845  3772    13 ...    13   406 10102]\n",
      " ...\n",
      " [ 5412   351   663 ...  1641  1816   284]\n",
      " [  198     1    40 ...   673  2993   340]\n",
      " [  262 17598   329 ...  1243    13   632]]\n",
      "(8, 128)\n",
      "Iteration 19/100 | Loss: 10.825837135314941\n",
      "[[   13 25860 26280 ... 13488   329   340]\n",
      " [  351   465   285 ... 22746   373   523]\n",
      " [34691   262  6842 ...   279  8918   319]\n",
      " ...\n",
      " [ 2121    13  1119 ...   466   407   804]\n",
      " [  607  1413  6041 ...  4071  1413    13]\n",
      " [ 1263    11 35075 ...   262 47565    13]]\n",
      "(8, 128)\n",
      "Iteration 20/100 | Loss: 10.825836181640625\n",
      "[[24829   284   262 ...  1110   339  1718]\n",
      " [   40  1101  7926 ...   679  5679 32189]\n",
      " [  262 37413    13 ...   198  3198  1110]\n",
      " ...\n",
      " [20037    13  1375 ... 11679   475  7247]\n",
      " [20037    13   314 ...   339  2497 20037]\n",
      " [ 1139    11   366 ...  5381    13  3244]]\n",
      "(8, 128)\n",
      "Iteration 21/100 | Loss: 10.825837135314941\n",
      "[[   11 20037  1464 ... 17608    13   314]\n",
      " [   11  6510    13 ... 29252    11   460]\n",
      " [  290  2952 45230 ...   198   198   464]\n",
      " ...\n",
      " [ 3260   326    11 ...    13  1375  3332]\n",
      " [  547   523  6568 ...  1816  3892   284]\n",
      " [  607 14142  3290 ...   379   262 10481]]\n",
      "(8, 128)\n",
      "Iteration 22/100 | Loss: 10.825836181640625\n",
      "[[  523  3772   290 ...   220   198   198]\n",
      " [   11  1037   502 ...  5667    13  1119]\n",
      " [  607 16825   284 ...   373  2048   355]\n",
      " ...\n",
      " [ 1119  1816   284 ...  9845   477  1110]\n",
      " [  257   640   612 ...  7577  9009   992]\n",
      " [  290  1422   470 ...  4030   607  3835]]\n",
      "(8, 128)\n",
      "Iteration 23/100 | Loss: 10.825836181640625\n",
      "[[ 3329    11   262 ...  5044   287   262]\n",
      " [  319   663  1182 ...   284  3002   607]\n",
      " [  262  8701    88 ...  4203 14066   329]\n",
      " ...\n",
      " [ 1165   526   198 ...    43   813 19952]\n",
      " [ 2576 18177 15063 ...    13  5811  1422]\n",
      " [  775  1839   470 ...   881  1257    13]]\n",
      "(8, 128)\n",
      "Iteration 24/100 | Loss: 10.825836181640625\n",
      "[[  632   373   262 ...   198   198 14967]\n",
      " [ 1807   340   373 ...   550   284   466]\n",
      " [  286   606  2067 ...  1271   286  1180]\n",
      " ...\n",
      " [13513    13  1119 ...   921 15063   257]\n",
      " [26926   531   340 ...  6568   290   531]\n",
      " [ 1995  1820  1816 ...   477  3332   866]]\n",
      "(8, 128)\n",
      "Iteration 25/100 | Loss: 10.825836181640625\n",
      "[[ 1497    13   887 ...    13  1119   550]\n",
      " [   13   887   355 ...   366   568   314]\n",
      " [  550   617 19533 ...   290   673   373]\n",
      " ...\n",
      " [11904   351   606 ...  3932   290 20037]\n",
      " [ 4186    13   632 ...  5812   645    11]\n",
      " [   30   843  3025 ...    13   679   815]]\n",
      "(8, 128)\n",
      "Iteration 26/100 | Loss: 10.825836181640625\n",
      "[[  679  6151   284 ...    13 44275   373]\n",
      " [   13  1881  1110 ...  1881  3024  3931]\n",
      " [  470   765   284 ...   262 33158    11]\n",
      " ...\n",
      " [12703    11  3387 ...   257  1263 12768]\n",
      " [ 3088   284 23529 ...  1363   351   257]\n",
      " [ 2712    11 20037 ...    13   198   198]]\n",
      "(8, 128)\n",
      "Iteration 27/100 | Loss: 10.825837135314941\n",
      "[[ 2067   284  6290 ... 23084  2227   284]\n",
      " [  644   340   531 ...   714    13 50256]\n",
      " [  423   257  1256 ...  6290 49253  7463]\n",
      " ...\n",
      " [40026  2354    13 ... 20037  2936  3772]\n",
      " [  198   198  2215 ...    13 24975    11]\n",
      " [  467  1363    13 ...   679  2993   340]]\n",
      "(8, 128)\n",
      "Iteration 28/100 | Loss: 10.825836181640625\n",
      "[[  290   484  2826 ...   351   465  2460]\n",
      " [  867  4695    13 ...   262  3952   290]\n",
      " [  510    13  1119 ...    11   262  7150]\n",
      " ...\n",
      " [  262  9955  6810 ...    13 17083   484]\n",
      " [  373   319   257 ...    11   339  6810]\n",
      " [ 1110    13  1119 ...   477  1816  1363]]\n",
      "(8, 128)\n",
      "Iteration 29/100 | Loss: 10.825836181640625\n",
      "[[ 1816   284   262 ...  7331 12403   287]\n",
      " [  198   198  3260 ...    13  1375  2936]\n",
      " [ 1257   290  6487 ... 20037   531    13]\n",
      " ...\n",
      " [ 1965    13   198 ...    11   703   546]\n",
      " [ 1775   878    13 ...   198  1544  4966]\n",
      " [  290 17293    11 ...   679  1965   262]]\n",
      "(8, 128)\n",
      "Iteration 30/100 | Loss: 10.825834274291992\n",
      "[[  373  6613   286 ... 14789   838    13]\n",
      " [  878  1804   606 ...   198   198 38582]\n",
      " [ 1816   866    11 ...   366    54 37711]\n",
      " ...\n",
      " [ 3638   262  3024 ...   257  2933    13]\n",
      " [29252    11   804 ...   257  3621 14540]\n",
      " [14274 46288   340 ...   339   561  1239]]\n",
      "(8, 128)\n",
      "Iteration 31/100 | Loss: 10.825835227966309\n",
      "[[ 8848    13   632 ...   220   198   198]\n",
      " [  407  5594   284 ...  1375   373 16755]\n",
      " [ 1110    11  4502 ...   550   257  1402]\n",
      " ...\n",
      " [   13   921   389 ...  2474 11735  1139]\n",
      " [ 1978    13 50256 ...   326   262 18328]\n",
      " [  198     1 13300 ...  2497   262  7988]]\n",
      "(8, 128)\n",
      "Iteration 32/100 | Loss: 10.825836181640625\n",
      "[[  921   460  3853 ...    11   475   484]\n",
      " [ 3091  1497    13 ...   257  3091   290]\n",
      " [ 2474   511  1995 ...  3952   790  1110]\n",
      " ...\n",
      " [   13   198   198 ...   257  2041  4365]\n",
      " [50256  7554   290 ...   284   766   644]\n",
      " [  711   287   262 ...   640    11   287]]\n",
      "(8, 128)\n",
      "Iteration 33/100 | Loss: 10.825836181640625\n",
      "[[   11   257  1310 ...  1327    13  1119]\n",
      " [  314  1101  1016 ...  2354   287   262]\n",
      " [11917   373  1464 ...   284   467   736]\n",
      " ...\n",
      " [   13   220   198 ...  3772    13 50256]\n",
      " [  290   673 14028 ...   287   262  4252]\n",
      " [  340   373   257 ... 14967    11  1745]]\n",
      "(8, 128)\n",
      "Iteration 34/100 | Loss: 10.825836181640625\n",
      "[[ 1513   683 26625 ...   366 17250    11]\n",
      " [  287   262 31488 ...   679   531   366]\n",
      " [  649   284  1282 ...  1528   517  1257]\n",
      " ...\n",
      " [  262 35845    13 ...   826  1497    13]\n",
      " [  511  1110   379 ...   510   477   262]\n",
      " [  640    11   612 ...   198   464  6512]]\n",
      "(8, 128)\n",
      "Iteration 35/100 | Loss: 10.825834274291992\n",
      "[[ 5045  1820   531 ...  3772   290  8072]\n",
      " [  465   898    13 ...     0   679   550]\n",
      " [ 1949 40170  1165 ...   262  8222  3436]\n",
      " ...\n",
      " [ 2241    13   679 ...   340   373  2818]\n",
      " [  475   350 10757 ...  5282   508  1464]\n",
      " [13300  1223  2041 ...   257  8119 11483]]\n",
      "(8, 128)\n",
      "Iteration 36/100 | Loss: 10.825835227966309\n",
      "[[ 9008   262  2323 ...   290  6810   340]\n",
      " [ 1375  5071   517 ... 11376   290   373]\n",
      " [ 3887  1110   339 ...  1394   683  3338]\n",
      " ...\n",
      " [ 3797    13   679 ... 31160   318  3772]\n",
      " [35881    13  1119 ...    13  1119   547]\n",
      " [  286   266 48501 ...   198   198    50]]\n",
      "(8, 128)\n",
      "Iteration 37/100 | Loss: 10.825835227966309\n",
      "[[    1  3123 15478 ... 35484     0   921]\n",
      " [ 1119  2921   683 ...   262 22155   284]\n",
      " [ 1111 13176   257 ...  1870 19217  1243]\n",
      " ...\n",
      " [  262  8701    13 ...   284   766   683]\n",
      " [  553   531   465 ...    13  1881  1110]\n",
      " [18116   780   673 ...   612   373   257]]\n",
      "(8, 128)\n",
      "Iteration 38/100 | Loss: 10.825834274291992\n",
      "[[   13  1119 22870 ...  2949    11 32189]\n",
      " [ 1517    13   921 ...  3198  1110    11]\n",
      " [  290   531   366 ...  2826  1978   329]\n",
      " ...\n",
      " [  287   257 16723 ...   307  1611   284]\n",
      " [28738   319   262 ...   300  9484   262]\n",
      " [ 5045  1820    13 ...  1820  3114   503]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m x, y \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Perform a single training step\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m model, optimizer_state, losses \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Log the loss to WandB and print progress\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# wandb.log({\"loss\": loss, \"iteration\": local_iter_num})\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[94], line 31\u001b[0m, in \u001b[0;36mmake_step\u001b[0;34m(model, optimizer_state, x, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_step\u001b[39m(model, optimizer_state, x, y):\n\u001b[1;32m     30\u001b[0m     losses, grads \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mfilter_value_and_grad(loss)(model, x, y)\n\u001b[0;32m---> 31\u001b[0m     updates, optimizer_state \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     model \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mapply_updates(model, updates)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, optimizer_state, losses\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/optax/schedules/_inject.py:196\u001b[0m, in \u001b[0;36minject_hyperparams.<locals>.wrapped_transform.<locals>.update_fn\u001b[0;34m(updates, state, params, **extra_args)\u001b[0m\n\u001b[1;32m    185\u001b[0m hparams\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m    186\u001b[0m     k: _convert_floats(\n\u001b[1;32m    187\u001b[0m         f(state\u001b[38;5;241m.\u001b[39mhyperparams_states[k], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args), dtype\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, f \u001b[38;5;129;01min\u001b[39;00m sched_hps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    190\u001b[0m })\n\u001b[1;32m    191\u001b[0m hyperparams_states \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    192\u001b[0m     k: f\u001b[38;5;241m.\u001b[39mupdate(state\u001b[38;5;241m.\u001b[39mhyperparams_states[k], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, f \u001b[38;5;129;01min\u001b[39;00m sched_hps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    194\u001b[0m }\n\u001b[0;32m--> 196\u001b[0m updates, inner_state \u001b[38;5;241m=\u001b[39m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_extra_args_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_hps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updates, InjectStatefulHyperparamsState(\n\u001b[1;32m    201\u001b[0m     count\u001b[38;5;241m=\u001b[39mnumerics\u001b[38;5;241m.\u001b[39msafe_increment(state\u001b[38;5;241m.\u001b[39mcount),\n\u001b[1;32m    202\u001b[0m     hyperparams\u001b[38;5;241m=\u001b[39mhparams,\n\u001b[1;32m    203\u001b[0m     hyperparams_states\u001b[38;5;241m=\u001b[39mhyperparams_states,\n\u001b[1;32m    204\u001b[0m     inner_state\u001b[38;5;241m=\u001b[39minner_state,\n\u001b[1;32m    205\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/optax/transforms/_combining.py:75\u001b[0m, in \u001b[0;36mchain.<locals>.update_fn\u001b[0;34m(updates, state, params, **extra_args)\u001b[0m\n\u001b[1;32m     73\u001b[0m new_state \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(state, update_fns):\n\u001b[0;32m---> 75\u001b[0m   updates, new_s \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m   new_state\u001b[38;5;241m.\u001b[39mappend(new_s)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updates, \u001b[38;5;28mtuple\u001b[39m(new_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/optax/_src/base.py:333\u001b[0m, in \u001b[0;36mwith_extra_args_support.<locals>.update\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(updates, state, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args):\n\u001b[1;32m    332\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m extra_args\n\u001b[0;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/optax/_src/transform.py:284\u001b[0m, in \u001b[0;36mscale_by_adam.<locals>.update_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fn\u001b[39m(updates, state, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    283\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m params\n\u001b[0;32m--> 284\u001b[0m   mu \u001b[38;5;241m=\u001b[39m \u001b[43motu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_update_moment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m   nu \u001b[38;5;241m=\u001b[39m otu\u001b[38;5;241m.\u001b[39mtree_update_moment_per_elem_norm(updates, state\u001b[38;5;241m.\u001b[39mnu, b2, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    286\u001b[0m   count_inc \u001b[38;5;241m=\u001b[39m numerics\u001b[38;5;241m.\u001b[39msafe_increment(state\u001b[38;5;241m.\u001b[39mcount)\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/optax/tree_utils/_tree_math.py:306\u001b[0m, in \u001b[0;36mtree_update_moment\u001b[0;34m(updates, moments, decay, order)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_update_moment\u001b[39m(updates, moments, decay, order):\n\u001b[1;32m    305\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the exponential moving average of the `order`-th moment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m          \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    309\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m      \u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmoments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m      \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/jax/_src/tree.py:155\u001b[0m, in \u001b[0;36mmap\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(f: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[1;32m    116\u001b[0m         tree: Any,\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;241m*\u001b[39mrest: Any,\n\u001b[1;32m    118\u001b[0m         is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    - :func:`jax.tree.reduce`\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/jax/_src/tree_util.py:344\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    342\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[1;32m    343\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/jax/_src/tree_util.py:344\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    342\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[1;32m    343\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/optax/tree_utils/_tree_math.py:308\u001b[0m, in \u001b[0;36mtree_update_moment.<locals>.<lambda>\u001b[0;34m(g, t)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_update_moment\u001b[39m(updates, moments, decay, order):\n\u001b[1;32m    305\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the exponential moving average of the `order`-th moment.\"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    307\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m g, t: (\n\u001b[0;32m--> 308\u001b[0m           (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m decay) \u001b[38;5;241m*\u001b[39m (\u001b[43mg\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43morder\u001b[49m) \u001b[38;5;241m+\u001b[39m decay \u001b[38;5;241m*\u001b[39m t \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    309\u001b[0m       ),\n\u001b[1;32m    310\u001b[0m       updates,\n\u001b[1;32m    311\u001b[0m       moments,\n\u001b[1;32m    312\u001b[0m       is_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    313\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:2472\u001b[0m, in \u001b[0;36mpower\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m   2470\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   2471\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2472\u001b[0m     x1, \u001b[38;5;241m=\u001b[39m \u001b[43mpromote_dtypes_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39minteger_pow(x1, x2)\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;66;03m# Handle cases #2 and #3 under a jit:\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/jax/_src/numpy/util.py:278\u001b[0m, in \u001b[0;36mpromote_dtypes_numeric\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function to apply Numpy argument dtype promotion.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03mPromotes arguments to a numeric (non-bool) type.\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m to_dtype, weak_type \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39m_lattice_result_type(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 278\u001b[0m to_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m to_dtype_numeric \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mto_numeric_dtype(to_dtype)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [lax\u001b[38;5;241m.\u001b[39m_convert_element_type(x, to_dtype_numeric, weak_type)\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/jax/_src/dtypes.py:257\u001b[0m, in \u001b[0;36mcanonicalize_dtype\u001b[0;34m(dtype, allow_extended_dtype)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;129m@export\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcanonicalize_dtype\u001b[39m(dtype: Any, allow_extended_dtype: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DType \u001b[38;5;241m|\u001b[39m ExtendedDType:\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Convert from a dtype to a canonical dtype based on config.x64_enabled.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_canonicalize_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_x64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_extended_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "key = jax.random.PRNGKey(69)\n",
    "\n",
    "gptconf = GPTConfig()\n",
    "model = GPT(gptconf, key)\n",
    "\n",
    "wandb.init(project=\"gpt-training\", config=gptconf.__dict__)\n",
    "\n",
    "optimizer_state = optimizer.init(model)\n",
    "num_iterations = 100\n",
    "\n",
    "for local_iter_num in range(num_iterations):\n",
    "    x, y = get_batch(\"train\")\n",
    "\n",
    "    # Perform a single training step\n",
    "    model, optimizer_state, losses = make_step(model, optimizer_state, x, y)\n",
    "\n",
    "    wandb.log({\"loss\": losses, \"iteration\": local_iter_num})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
