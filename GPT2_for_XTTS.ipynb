{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a GPT implementation shadowing HF's implementation of GPT2. It might be broken or noto work, I haven't checked much.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by importing our favorite libraries ü•∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "import jax.numpy as jnp\n",
    "import typing as tp\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activation function\n",
    "\n",
    "XTTS uses HF's transformer underneath. This GPT uses the GLU_new activation function: \n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from jaxtyping import ArrayLike\n",
    "import math\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def glu_new(x: ArrayLike) -> jax.Array:\n",
    "    return jax.numpy.array(\n",
    "        0.5\n",
    "        * x\n",
    "        * (\n",
    "            1\n",
    "            + jax.numpy.tanh(\n",
    "                math.sqrt(2.0 / math.pi) * (x + 0.044715 * jax.numpy.pow(x, 3.0))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def forward(input: Tensor) -> Tensor:\n",
    "    return (\n",
    "        0.5\n",
    "        * input\n",
    "        * (\n",
    "            1.0\n",
    "            + torch.tanh(\n",
    "                math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if they're the same below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "import numpy\n",
    "\n",
    "key = jax.random.PRNGKey(69)\n",
    "hidden_states = jax.random.normal(key, (100))\n",
    "tor = torch.from_numpy(numpy.array(hidden_states))\n",
    "\n",
    "attn_output = softplus(hidden_states)\n",
    "ytor = forward(tor)\n",
    "\n",
    "dif = ytor - torch.from_numpy(numpy.array(attn_output))\n",
    "print(dif)\n",
    "# assert torch.testing.assert_close(\n",
    "#     ytor, torch.from_numpy(numpy.array(softplus(x)))\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONV1d\n",
    "\n",
    "They have a strange conv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class our_Conv1D(eqx.Module):\n",
    "    \"\"\"\n",
    "    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n",
    "\n",
    "    Basically works like a linear layer but the weights are transposed.\n",
    "\n",
    "    Args:\n",
    "        nf (`int`): The number of output features.\n",
    "        nx (`int`): The number of input features.\n",
    "    \"\"\"\n",
    "\n",
    "    nf: int\n",
    "    nx: int\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, nf, nx, key=None):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        self.nx = nx\n",
    "        self.weight = jax.nn.initializers.normal(stddev=0.02)(key, (nx, nf))\n",
    "        self.bias = jax.numpy.zeros((nf))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Conv1D(nf={nf}, nx={nx})\".format(**self.__dict__)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        size_out = x.shape[:-1] + (self.nf,)\n",
    "        x = self.bias + jax.numpy.dot(\n",
    "            jax.numpy.reshape(x, shape=(-1, x.shape[-1])), self.weight\n",
    "        )\n",
    "        return jax.numpy.reshape(x, size_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "input_dim = 1024\n",
    "output_dim = 4096\n",
    "\n",
    "their_gpt = transformers.pytorch_utils.Conv1D(output_dim, input_dim)\n",
    "our_gpt = our_Conv1D(output_dim, input_dim, jax.random.PRNGKey(1))\n",
    "\n",
    "print(our_gpt.bias.shape)\n",
    "\n",
    "our_x = jax.random.normal(jax.random.PRNGKey(1), shape=(100, 1024))\n",
    "their_x = torch.from_numpy(numpy.array(our_x))\n",
    "\n",
    "print(list(their_gpt.named_parameters()))\n",
    "\n",
    "\n",
    "torch_params = {\n",
    "    name: param.detach().numpy() for name, param in their_gpt.named_parameters()\n",
    "}\n",
    "\n",
    "torch_to_jax_keys = {\n",
    "    (\"weight\", \"weight\"),\n",
    "    (\"bias\", \"bias\"),\n",
    "}\n",
    "\n",
    "\n",
    "# Function to update the JAX model parameters\n",
    "def update_params(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    for jax_key, torch_key in torch_to_jax_keys:\n",
    "        if jax_key == path:\n",
    "            print(jax_key)\n",
    "            if \"bias\" in jax_key:\n",
    "                return jax.numpy.array(torch_params[torch_key])\n",
    "            return jax.numpy.array(torch_params[torch_key])\n",
    "    return x\n",
    "\n",
    "\n",
    "our_gpt = jax.tree_util.tree_map_with_path(update_params, our_gpt)\n",
    "\n",
    "print(type(our_x))\n",
    "print(our_x.shape)\n",
    "their_y = their_gpt(their_x)\n",
    "print(their_y.size())\n",
    "our_y = our_gpt(our_x)\n",
    "\n",
    "torch.testing.assert_close(their_y, torch.from_numpy(numpy.array(our_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "We can now move onto the multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 128\n",
    "    vocab_size: int = (\n",
    "        50304  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    )\n",
    "    n_layer: int = 3\n",
    "    n_head: int = 3\n",
    "    n_embd: int = 200\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = False  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(eqx.Module):\n",
    "    c_fc: our_Conv1D\n",
    "    c_proj: our_Conv1D\n",
    "    dropout: nn.Dropout\n",
    "\n",
    "    def __init__(self, intermediate_size, config, key):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "\n",
    "        # The weights are transposed compraed to the feed forward.\n",
    "        embed_dim = config.hidden_size\n",
    "        self.c_fc = our_Conv1D(intermediate_size, embed_dim, key=key1)\n",
    "        self.c_proj = our_Conv1D(embed_dim, intermediate_size, key=key2)\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop, deterministic=True)\n",
    "\n",
    "    # TODO: Interesting take on the fact that vmap should be applied here ?\n",
    "    def __call__(self, x):\n",
    "        y = self.c_fc(x)\n",
    "        y = glu_new(y)\n",
    "        y = self.c_proj(y)\n",
    "        y = self.dropout(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't be sure of the last part as it uses dropout but the rest seems to work üëç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2MLP\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Config\n",
    "\n",
    "intermediate_size = 4096\n",
    "\n",
    "\n",
    "their_config = GPT2Config(hidden_size=1024)\n",
    "key = jax.random.PRNGKey(69)\n",
    "\n",
    "their_gpt = GPT2MLP(intermediate_size, their_config)\n",
    "our_gpt = MLP(intermediate_size, their_config, jax.random.PRNGKey(1))\n",
    "\n",
    "our_x = jax.random.normal(jax.random.PRNGKey(1), shape=(100, 1024))\n",
    "their_x = torch.from_numpy(numpy.array(our_x))\n",
    "\n",
    "torch_params = {\n",
    "    name: param.detach().numpy() for name, param in their_gpt.named_parameters()\n",
    "}\n",
    "\n",
    "print(torch_params.keys())\n",
    "\n",
    "torch_to_jax_keys = {\n",
    "    (\"c_fc.weight\", \"c_fc.weight\"),\n",
    "    (\"c_fc.bias\", \"c_fc.bias\"),\n",
    "    (\"c_proj.weight\", \"c_proj.weight\"),\n",
    "    (\"c_proj.bias\", \"c_proj.bias\"),\n",
    "}\n",
    "\n",
    "\n",
    "# Function to update the JAX model parameters\n",
    "def update_params(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    for jax_key, torch_key in torch_to_jax_keys:\n",
    "        if jax_key == path:\n",
    "            print(jax_key)\n",
    "            if \"bias\" in jax_key:\n",
    "                return jax.numpy.array(torch_params[torch_key])\n",
    "            return jax.numpy.array(torch_params[torch_key])\n",
    "    return x\n",
    "\n",
    "\n",
    "our_gpt = jax.tree_util.tree_map_with_path(update_params, our_gpt)\n",
    "\n",
    "print(type(our_x))\n",
    "print(our_x.shape)\n",
    "their_y = their_gpt.c_fc(their_x)\n",
    "their_y = their_gpt.act(their_y)\n",
    "their_y = their_gpt.c_proj(their_y)\n",
    "# their_y = their_gpt.dropout(their_y)\n",
    "print(their_y.size())\n",
    "our_y = our_gpt.c_fc(our_x)\n",
    "our_y = glu_new(our_y)\n",
    "our_y = our_gpt.c_proj(our_y)\n",
    "# our_y = our_gpt.dropout(our_y)\n",
    "\n",
    "torch.testing.assert_close(their_y, torch.from_numpy(numpy.array(our_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the Conv1D isn't the same for the GPT - testing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from equinox.nn import Conv1d\n",
    "\n",
    "hidden_states = jax.numpy.ones((10, 5))\n",
    "\n",
    "torch_covn = our_Conv1D(7, 10)\n",
    "\n",
    "jax_conv = Conv1d(10, 7, key=jax.random.PRNGKey(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can compare with their implementation to make sure we're close enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SwiGLU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# | code-fold : true\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMLPTheirs\u001b[39;00m(eqx\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      5\u001b[0m     c_fc: eqx\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear\n\u001b[1;32m      6\u001b[0m     swiglu: SwiGLU\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mMLPTheirs\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMLPTheirs\u001b[39;00m(eqx\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      5\u001b[0m     c_fc: eqx\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear\n\u001b[0;32m----> 6\u001b[0m     swiglu: \u001b[43mSwiGLU\u001b[49m\n\u001b[1;32m      7\u001b[0m     c_proj: eqx\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear\n\u001b[1;32m      8\u001b[0m     dropout: eqx\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SwiGLU' is not defined"
     ]
    }
   ],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "\n",
    "class MLPTheirs(eqx.Module):\n",
    "    c_fc: eqx.nn.Linear\n",
    "    swiglu: SwiGLU\n",
    "    c_proj: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "\n",
    "    def __init__(self, config, key):\n",
    "        lkey1, lkey2, skey = jax.random.split(key, 3)\n",
    "\n",
    "        self.c_fc = eqx.nn.Linear(\n",
    "            config.n_embd, 4 * config.n_embd, use_bias=config.bias, key=lkey1\n",
    "        )\n",
    "        self.swiglu = SwiGLU(4 * config.n_embd, 4 * config.n_embd, skey)\n",
    "        self.c_proj = eqx.nn.Linear(\n",
    "            4 * config.n_embd, config.n_embd, use_bias=config.bias, key=lkey2\n",
    "        )\n",
    "        self.dropout = eqx.nn.Dropout(config.dropout)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = jax.vmap(self.c_fc)(x)\n",
    "        x = jax.vmap(self.swiglu)(x)\n",
    "        x = jax.vmap(self.c_proj)(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "\n",
    "their_config = GPTConfig()\n",
    "key = jax.random.PRNGKey(69)\n",
    "\n",
    "mlp = MLP(their_config, key)\n",
    "mlp_theirs = MLPTheirs(their_config, key)\n",
    "\n",
    "hidden_states = jax.random.normal(key, (100, their_config.n_embd))\n",
    "\n",
    "res = jax.vmap(mlp)(hidden_states)\n",
    "res_theirs = mlp_theirs(hidden_states)\n",
    "\n",
    "average_diff = jnp.mean(res_theirs)\n",
    "print(average_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked attention\n",
    "\n",
    "Moving onto one of the more complicated aspects of the model, but in the end it simply learns to output which tokens are more important with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "import jax\n",
    "import jax.experimental\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class CausalSelfAttention(eqx.Module):\n",
    "    c_attn: our_Conv1D\n",
    "    c_proj: our_Conv1D\n",
    "\n",
    "    resid_dropout: nn.Dropout\n",
    "    attn_dropout: nn.Dropout\n",
    "\n",
    "    bias: jax.Array = eqx.field(static=True)\n",
    "    scale_attn_weights: bool\n",
    "    split_size: int\n",
    "\n",
    "    num_heads: int\n",
    "    head_size: int\n",
    "\n",
    "    def __init__(self, config, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "\n",
    "        hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_size = hidden_size // config.num_attention_heads\n",
    "        self.split_size = hidden_size\n",
    "\n",
    "        self.c_attn = our_Conv1D(3 * hidden_size, hidden_size, key=key1)\n",
    "        self.c_proj = our_Conv1D(hidden_size, hidden_size, key=key3)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop, deterministic=True)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop, deterministic=True)\n",
    "\n",
    "        self.bias = jnp.tril(\n",
    "            jnp.ones(\n",
    "                (1, 1, config.max_position_embeddings, config.max_position_embeddings)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.scale_attn_weights = config.scale_attn_weights\n",
    "\n",
    "    # Could play arround with the different attention score calculations (Baidhu ?)\n",
    "    # X is an embedding, it should self attend.\n",
    "\n",
    "    def _attn(self, q, k, v, attention_mask, head_mask):\n",
    "        att = jnp.matmul(q, jnp.transpose(k, axes=(0, 1, 3, 2)))\n",
    "        att = att / math.sqrt(jnp.shape(k)[-1])  # Scale weights is set to true in XTTS.\n",
    "\n",
    "        query_length, key_length = q.shape[-2], k.shape[-2]\n",
    "        mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "        att = jnp.where(\n",
    "            jax.numpy.equal(jax.lax.stop_gradient(mask), 0),\n",
    "            jnp.finfo(att.dtype).min,\n",
    "            att,\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            att = att + attention_mask\n",
    "\n",
    "        att = jax.nn.softmax(att, axis=-1)\n",
    "\n",
    "        # att = self.attn_dropout(att)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            att = att * head_mask\n",
    "\n",
    "        return jnp.matmul(att, v), att\n",
    "\n",
    "    # Stange that they do it this way and not by simply defining the dims without permutation\n",
    "    def _split_heads(self, x):\n",
    "        new_shape = x.shape[:-1] + (self.num_heads, self.head_size)\n",
    "        x = jax.numpy.reshape(x, new_shape)\n",
    "        return jax.numpy.permute_dims(x, (0, 2, 1, 3))\n",
    "\n",
    "    def _merge_heads(self, x):\n",
    "        x = jax.numpy.permute_dims(x, (0, 2, 1, 3))\n",
    "        new_shape = x.shape[:-2] + (self.num_heads * self.head_size,)\n",
    "        return jax.numpy.reshape(x, new_shape)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        hidden_states: tp.Optional[tp.Tuple[jax.Array]],\n",
    "        layer_past: tp.Optional[tp.Tuple[jax.Array]] = None,\n",
    "        attention_mask: tp.Optional[jax.Array] = None,\n",
    "        head_mask: tp.Optional[jax.Array] = None,\n",
    "        encoder_hidden_states: tp.Optional[jax.Array] = None,\n",
    "        encoder_attention_mask: tp.Optional[jax.Array] = None,\n",
    "        use_cache: tp.Optional[bool] = False,\n",
    "        output_attentions: tp.Optional[bool] = False,\n",
    "    ):\n",
    "        print(f\"SHAPE OF {hidden_states.shape}\")\n",
    "        # x = jnp.swapaxes(x, -1, -2)\n",
    "        qkv = jax.vmap(self.c_attn)(hidden_states)\n",
    "        q, k, v = jax.numpy.split(qkv, 3, axis=2)\n",
    "\n",
    "        query = self._split_heads(q)\n",
    "        key = self._split_heads(k)\n",
    "        value = self._split_heads(v)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key = layer_past[0]\n",
    "            past_value = layer_past[1]\n",
    "            key = jax.numpy.concat((past_key, key), axis=-2)\n",
    "            value = jax.numpy.concat((past_value, value), axis=-2)\n",
    "\n",
    "        present = None\n",
    "        if use_cache is True:\n",
    "            present = (key, value)\n",
    "\n",
    "        # print(query.shape)\n",
    "        attn_output, attn_weights = self._attn(\n",
    "            query, key, value, attention_mask, head_mask\n",
    "        )\n",
    "        attn_output = self._merge_heads(attn_output)\n",
    "        attn_output = jax.vmap(self.c_proj)(attn_output)\n",
    "        # attn_output = self.resid_dropout(attn_output)\n",
    "        print(f\"Ours: {attn_output.shape}\")\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tugdual/miniconda3/envs/xtts/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/tugdual/miniconda3/envs/xtts/lib/python3.10/site-packages/equinox/nn/_dropout.py:45: UserWarning: Dropout(deterministic=...) is deprecated in favour of Dropout(inference=...)\n",
      "  warnings.warn(\n",
      "/var/folders/79/hdfw6x594jbbkfl6hzwf8cxw0000gn/T/ipykernel_23457/126106335.py:49: UserWarning: A JAX array is being set as static! This can result in unexpected behavior and is usually a mistake to do.\n",
      "  our_gpt = CausalSelfAttention(their_config, jax.random.PRNGKey(1))\n",
      "`GPT2SdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['c_attn.weight', 'c_attn.bias', 'c_proj.weight', 'c_proj.bias'])\n",
      "c_attn.weight\n",
      "c_attn.bias\n",
      "c_proj.weight\n",
      "c_proj.bias\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "(10, 1082, 1024)\n",
      "Theirs: torch.Size([10, 1082, 1024])\n",
      "SHAPE OF (10, 1082, 1024)\n",
      "Ours: (10, 1082, 1024)\n",
      "[ 0.96926224  0.0685735  -0.2757392  ...  0.19498248 -0.12266847\n",
      "  0.26153973]\n",
      "tensor([ 0.9693,  0.0686, -0.2757,  ...,  0.1950, -0.1227,  0.2615],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# | code-fold : true\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2SdpaAttention\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "import jax, torch, numpy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Config\n",
    "\n",
    "\n",
    "intermediate_size = 4096\n",
    "\n",
    "\n",
    "their_config = GPT2Config().from_dict(\n",
    "    {\n",
    "        \"_attn_implementation_autoset\": True,\n",
    "        \"activation_function\": \"gelu_new\",\n",
    "        \"attn_pdrop\": 0.1,\n",
    "        \"bos_token_id\": 50256,\n",
    "        \"embd_pdrop\": 0.1,\n",
    "        \"eos_token_id\": 50256,\n",
    "        \"gradient_checkpointing\": False,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"layer_norm_epsilon\": 1e-05,\n",
    "        \"model_type\": \"gpt2\",\n",
    "        \"n_ctx\": 1082,\n",
    "        \"n_embd\": 1024,\n",
    "        \"n_head\": 16,\n",
    "        \"n_inner\": None,\n",
    "        \"n_layer\": 30,\n",
    "        \"n_positions\": 1082,\n",
    "        \"reorder_and_upcast_attn\": False,\n",
    "        \"resid_pdrop\": 0.1,\n",
    "        \"scale_attn_by_inverse_layer_idx\": False,\n",
    "        \"scale_attn_weights\": True,\n",
    "        \"summary_activation\": None,\n",
    "        \"summary_first_dropout\": 0.1,\n",
    "        \"summary_proj_to_labels\": True,\n",
    "        \"summary_type\": \"cls_index\",\n",
    "        \"summary_use_proj\": True,\n",
    "        \"transformers_version\": \"4.46.2\",\n",
    "        \"use_cache\": True,\n",
    "        \"vocab_size\": 256,\n",
    "    }\n",
    ")\n",
    "\n",
    "key = jax.random.PRNGKey(9)\n",
    "\n",
    "their_gpt = GPT2SdpaAttention(their_config)\n",
    "our_gpt = CausalSelfAttention(their_config, jax.random.PRNGKey(1))\n",
    "\n",
    "our_x = jax.random.normal(jax.random.PRNGKey(1), shape=(10, 1082, 1024))\n",
    "their_x = torch.from_numpy(numpy.array(our_x))\n",
    "\n",
    "# their_head_mask = torch.zeros((1, 1, 1024))\n",
    "# our_head_mask = jax.numpy.zeros((10, 1082, 1024))\n",
    "\n",
    "torch_params = {\n",
    "    name: param.detach().numpy() for name, param in their_gpt.named_parameters()\n",
    "}\n",
    "\n",
    "print(torch_params.keys())\n",
    "\n",
    "torch_to_jax_keys = {\n",
    "    (\"c_attn.weight\", \"c_attn.weight\"),\n",
    "    (\"c_attn.bias\", \"c_attn.bias\"),\n",
    "    (\"c_proj.weight\", \"c_proj.weight\"),\n",
    "    (\"c_proj.bias\", \"c_proj.bias\"),\n",
    "}\n",
    "\n",
    "\n",
    "# Function to update the JAX model parameters\n",
    "def update_params(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    for jax_key, torch_key in torch_to_jax_keys:\n",
    "        if jax_key == path:\n",
    "            print(jax_key)\n",
    "            if \"bias\" in jax_key:\n",
    "                return jax.numpy.array(torch_params[torch_key])\n",
    "            return jax.numpy.array(torch_params[torch_key])\n",
    "    return x\n",
    "\n",
    "\n",
    "our_gpt = jax.tree_util.tree_map_with_path(update_params, our_gpt)\n",
    "\n",
    "print(type(our_x))\n",
    "print(our_x.shape)\n",
    "their_y = their_gpt(their_x, output_attentions=True)\n",
    "our_y = our_gpt(our_x)\n",
    "print(our_y[0][0, 0])\n",
    "print(their_y[0][0, 0])\n",
    "torch.testing.assert_close(their_y[0], torch.from_numpy(numpy.array(our_y[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "our_x = jax.random.normal(jax.random.PRNGKey(1), shape=(3, 100, 10))\n",
    "their_x = torch.from_numpy(numpy.array(our_x))\n",
    "\n",
    "their_y = F.scaled_dot_product_attention(\n",
    "    their_x[0], their_x[1], their_x[2], is_causal=True\n",
    ")\n",
    "\n",
    "our_y = jax.nn.dot_product_attention(\n",
    "    our_x[0], our_x[1], our_x[2], is_causal=True, implementation=\"xla\"\n",
    ")\n",
    "\n",
    "print(their_y[0:10])\n",
    "print(our_y[0:10])\n",
    "torch.testing.assert_close(their_y, torch.from_numpy(numpy.array(our_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block\n",
    "\n",
    "Ok ! Now that we have the component parts of what we call a \"block\" we can assemble them. This will then be stacked to get as many layers of abstraction as we wish. In our case we will stack it 12 times as per the GPTConfig we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class Block(eqx.Module):\n",
    "    ln_1: nn.LayerNorm\n",
    "    ln_2: nn.LayerNorm\n",
    "    attn: CausalSelfAttention\n",
    "    mlp: MLP\n",
    "\n",
    "    def __init__(self, config, key):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "        hidden_size = config.hidden_size\n",
    "        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(\n",
    "            (hidden_size),\n",
    "            eps=config.layer_norm_epsilon,\n",
    "            elementwise_affine=True,\n",
    "        )\n",
    "        self.attn = CausalSelfAttention(config, key=key1)\n",
    "        self.ln_2 = nn.LayerNorm(\n",
    "            (hidden_size),\n",
    "            eps=config.layer_norm_epsilon,\n",
    "            elementwise_affine=True,\n",
    "        )\n",
    "\n",
    "        self.mlp = MLP(inner_dim, config, key=key2)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        hidden_states: tp.Optional[tp.Tuple[jax.Array]],\n",
    "        layer_past: tp.Optional[tp.Tuple[jax.Array]] = None,\n",
    "        attention_mask: tp.Optional[jax.Array] = None,\n",
    "        head_mask: tp.Optional[jax.Array] = None,\n",
    "        encoder_hidden_states: tp.Optional[jax.Array] = None,\n",
    "        encoder_attention_mask: tp.Optional[jax.Array] = None,\n",
    "        use_cache: tp.Optional[bool] = False,\n",
    "        output_attentions: tp.Optional[bool] = False,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "        hidden_states = jax.vmap(jax.vmap(self.ln_1))(hidden_states)\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )  # Can't vmap as the whole point is exchange info between tokens.\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]\n",
    "        hidden_states = attn_output + residual\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = jax.vmap(jax.vmap(self.ln_2))(hidden_states)\n",
    "        feed_forward_hidden_states = jax.vmap(self.mlp)(hidden_states)\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        if use_cache:\n",
    "            return (hidden_states,) + outputs\n",
    "        else:\n",
    "            return (hidden_states,) + outputs[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can compare with their work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from transformers import GPT2Config\n",
    "import jax, torch, numpy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Config\n",
    "\n",
    "\n",
    "their_config = GPT2Config().from_dict(\n",
    "    {\n",
    "        \"_attn_implementation_autoset\": True,\n",
    "        \"activation_function\": \"gelu_new\",\n",
    "        \"attn_pdrop\": 0.1,\n",
    "        \"bos_token_id\": 50256,\n",
    "        \"embd_pdrop\": 0.1,\n",
    "        \"eos_token_id\": 50256,\n",
    "        \"gradient_checkpointing\": False,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"layer_norm_epsilon\": 1e-05,\n",
    "        \"model_type\": \"gpt2\",\n",
    "        \"n_ctx\": 1082,\n",
    "        \"n_embd\": 1024,\n",
    "        \"n_head\": 16,\n",
    "        \"n_inner\": None,\n",
    "        \"n_layer\": 30,\n",
    "        \"n_positions\": 1082,\n",
    "        \"reorder_and_upcast_attn\": False,\n",
    "        \"resid_pdrop\": 0.1,\n",
    "        \"scale_attn_by_inverse_layer_idx\": False,\n",
    "        \"scale_attn_weights\": True,\n",
    "        \"summary_activation\": None,\n",
    "        \"summary_first_dropout\": 0.1,\n",
    "        \"summary_proj_to_labels\": True,\n",
    "        \"summary_type\": \"cls_index\",\n",
    "        \"summary_use_proj\": True,\n",
    "        \"transformers_version\": \"4.46.2\",\n",
    "        \"use_cache\": True,\n",
    "        \"vocab_size\": 256,\n",
    "    }\n",
    ")\n",
    "\n",
    "key = jax.random.PRNGKey(2)\n",
    "\n",
    "their_gpt = GPT2Block(their_config)\n",
    "our_gpt = Block(their_config, jax.random.PRNGKey(1))\n",
    "\n",
    "torch_params = {\n",
    "    name: param.detach().numpy() for name, param in their_gpt.named_parameters()\n",
    "}\n",
    "\n",
    "\n",
    "# Function to update the JAX model parameters\n",
    "def update_params(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    # for jax_key, torch_key in torch_to_jax_keys:\n",
    "    if path in torch_params.keys():\n",
    "        if \"bias\" in path:\n",
    "            return jax.numpy.array(torch_params[path])\n",
    "        return jax.numpy.array(torch_params[path])\n",
    "    print(path)\n",
    "    return x\n",
    "\n",
    "\n",
    "our_gpt = jax.tree_util.tree_map_with_path(update_params, our_gpt)\n",
    "\n",
    "our_x = jax.random.normal(jax.random.PRNGKey(2), shape=(10, 1082, 1024))\n",
    "their_x = torch.from_numpy(numpy.array(our_x))\n",
    "print(our_x[0, 0])\n",
    "print(their_x[0, 0])\n",
    "\n",
    "their_y = their_gpt(their_x)\n",
    "# (10, 16, 1082, 64) is the shape of the attentions\n",
    "our_y = our_gpt(our_x)\n",
    "torch.testing.assert_close(their_y[0], torch.from_numpy(numpy.array(our_y[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the three main parts that consitute the last part, the block have been coded, we can proceed to the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "\n",
    "class TransformerLayer(eqx.Module):\n",
    "    wte: nn.Embedding  # Token embeddings\n",
    "    wpe: nn.Embedding  # Positional embeddings\n",
    "\n",
    "    drop: nn.Dropout\n",
    "\n",
    "    embed_dim: int\n",
    "\n",
    "    h: list\n",
    "    norm: nn.LayerNorm\n",
    "\n",
    "    def __init__(self, config, key):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim, key=key1)\n",
    "        self.wpe = nn.Embedding(\n",
    "            config.max_position_embeddings, self.embed_dim, key=key2\n",
    "        )\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "        self.h = [\n",
    "            Block(config, y) for y in jax.random.split(key, config.num_hidden_layers)\n",
    "        ]\n",
    "        self.norm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids: tp.Optional[jax.Array] = None,  # One ID inputted ?\n",
    "        past_key_values: tp.Optional[jax.Array] = None,  # Used !\n",
    "        attention_mask: tp.Optional[jax.Array] = None,  # Used !\n",
    "        token_type_ids: tp.Optional[jax.Array] = None,  # Not used\n",
    "        position_ids: tp.Optional[jax.Array] = None,  # Used !\n",
    "        head_mask: tp.Optional[jax.Array] = None,  # Isn't used\n",
    "        inputs_embeds: tp.Optional[jax.Array] = None,  # Isn't used\n",
    "        output_attentions: tp.Optional[bool] = None,  # Isn't used\n",
    "        output_hidden_states: tp.Optional[bool] = None,  # Isn't used\n",
    "        use_cache: tp.Optional[bool] = False,  # Set to true.\n",
    "        return_dict: tp.Optional[bool] = False,  # Set to true.\n",
    "    ):\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n",
    "            )\n",
    "        elif input_ids is not None:\n",
    "            # self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
    "            input_shape = input_ids.shape\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.shape[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        # Should use better positional embeddings with cos and sin.\n",
    "        if past_key_values is None:\n",
    "            past_length = 0\n",
    "            past_key_values = tuple([None] * len(self.h))\n",
    "        else:\n",
    "            past_length = past_key_values[0].shape[-2]\n",
    "        if position_ids is None:\n",
    "            position_ids = jax.numpy.arange(past_length, input_shape[-1] + past_length)\n",
    "            position_ids = jax.numpy.expand_dims(position_ids, 0)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = jax.vmap(jax.vmap(self.wte))(input_ids)\n",
    "\n",
    "        # pos = jnp.arange(0, t, dtype=jnp.int64)\n",
    "\n",
    "        position_embeds = jax.vmap(jax.vmap(self.wpe))(position_ids)\n",
    "\n",
    "        # Dropout at the first layer ? Seems a bit aggressive...\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "\n",
    "        # No need for fancy stuff for the attention mask, simply since it's applied before the softmax change the values of 0 to -inf\n",
    "        if attention_mask is not None:\n",
    "\n",
    "            attention_mask = jax.numpy.where(\n",
    "                jax.numpy.equal(attention_mask, 1), 1, -jax.numpy.inf\n",
    "            )\n",
    "        # No cross attention so we're all good.\n",
    "        # No head mask.\n",
    "        # Token type ids is none\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "        # Not training.\n",
    "        presents = () if use_cache else None\n",
    "        # Output attentions not used\n",
    "        # no cross attention_mask\n",
    "        # No output hidden states\n",
    "        # print(f\"Ours : {hidden_states[0,0,:10]}\")\n",
    "\n",
    "        for block, layer_past in zip(self.h, past_key_values):\n",
    "            outputs = block(\n",
    "                hidden_states,\n",
    "                layer_past=layer_past,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=None,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            hidden_states = outputs[0]\n",
    "            # print(f\"Ours : {hidden_states[0,0,:10]}\")\n",
    "            if use_cache:\n",
    "                presents = presents + (outputs[1],)\n",
    "\n",
    "        hidden_states = jax.vmap(jax.vmap(self.norm))(hidden_states)\n",
    "\n",
    "        if return_dict:\n",
    "            return BaseModelOutputWithPastAndCrossAttentions(\n",
    "                last_hidden_state=hidden_states,\n",
    "                past_key_values=presents,\n",
    "                attentions=None,\n",
    "                cross_attentions=None,\n",
    "                hidden_states=None,\n",
    "            )\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with their work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold : true\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import jax, torch, numpy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Config\n",
    "\n",
    "\n",
    "their_config = GPT2Config(\n",
    "    vocab_size=256,  # Unused.\n",
    "    n_positions=1082,\n",
    "    n_ctx=1082,\n",
    "    n_embd=1024,\n",
    "    n_layer=30,\n",
    "    n_head=16,\n",
    "    gradient_checkpointing=False,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "key = jax.random.PRNGKey(2)\n",
    "\n",
    "their_gpt = GPT2Model(their_config)\n",
    "\n",
    "torch_params = {\n",
    "    name: param.detach().numpy() for name, param in their_gpt.named_parameters()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to update the JAX model parameters\n",
    "# def update_params(path, x):\n",
    "#     path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "#     # for jax_key, torch_key in torch_to_jax_keys:\n",
    "#     if path in torch_params.keys():\n",
    "#         if \"bias\" in path:\n",
    "#             return jax.numpy.array(torch_params[path])\n",
    "#         return jax.numpy.array(torch_params[path])\n",
    "#     print(path)\n",
    "#     return x\n",
    "\n",
    "\n",
    "jax.tree_util.tree_map_with_path(lambda path, _: print(path), our_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tugdual/miniconda3/envs/xtts/lib/python3.10/site-packages/equinox/nn/_normalisation.py:91: UserWarning: LayerNorm(elementwise_affine=...) is deprecated in favour of LayerNorm(use_weight=...) and LayerNorm(use_bias=...)\n",
      "  warnings.warn(\n",
      "/Users/tugdual/miniconda3/envs/xtts/lib/python3.10/site-packages/equinox/nn/_dropout.py:45: UserWarning: Dropout(deterministic=...) is deprecated in favour of Dropout(inference=...)\n",
      "  warnings.warn(\n",
      "/var/folders/79/hdfw6x594jbbkfl6hzwf8cxw0000gn/T/ipykernel_77409/1838700146.py:17: UserWarning: A JAX array is being set as static! This can result in unexpected behavior and is usually a mistake to do.\n",
      "  self.attn = CausalSelfAttention(config, key=key1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "tensor(5, dtype=torch.int32)\n",
      "Theirs : tensor([ 0.0346, -0.0105,  0.0182,  0.0283, -0.0087,  0.0409, -0.0246,  0.0024,\n",
      "        -0.0308, -0.0179], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0374, -0.0112,  0.0191,  0.0348, -0.0100,  0.0409, -0.0253, -0.0005,\n",
      "        -0.0327, -0.0191], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0371, -0.0128,  0.0212,  0.0345, -0.0107,  0.0429, -0.0272,  0.0021,\n",
      "        -0.0311, -0.0163], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0361, -0.0096,  0.0201,  0.0358, -0.0133,  0.0418, -0.0299,  0.0035,\n",
      "        -0.0295, -0.0158], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0351, -0.0097,  0.0204,  0.0386, -0.0165,  0.0390, -0.0302,  0.0072,\n",
      "        -0.0325, -0.0134], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0348, -0.0120,  0.0210,  0.0416, -0.0160,  0.0389, -0.0306,  0.0066,\n",
      "        -0.0322, -0.0124], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0368, -0.0152,  0.0201,  0.0385, -0.0154,  0.0401, -0.0299,  0.0042,\n",
      "        -0.0348, -0.0153], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0397, -0.0125,  0.0202,  0.0340, -0.0180,  0.0376, -0.0332,  0.0058,\n",
      "        -0.0320, -0.0151], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0395, -0.0125,  0.0211,  0.0324, -0.0204,  0.0445, -0.0300,  0.0064,\n",
      "        -0.0368, -0.0147], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0387, -0.0111,  0.0194,  0.0319, -0.0202,  0.0403, -0.0313,  0.0063,\n",
      "        -0.0377, -0.0139], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0372, -0.0099,  0.0237,  0.0323, -0.0193,  0.0420, -0.0299,  0.0029,\n",
      "        -0.0406, -0.0166], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0348, -0.0077,  0.0203,  0.0316, -0.0229,  0.0367, -0.0321, -0.0014,\n",
      "        -0.0391, -0.0153], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0391, -0.0015,  0.0236,  0.0350, -0.0260,  0.0384, -0.0303, -0.0052,\n",
      "        -0.0397, -0.0151], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0413, -0.0017,  0.0261,  0.0370, -0.0295,  0.0355, -0.0325, -0.0058,\n",
      "        -0.0380, -0.0181], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0409, -0.0021,  0.0303,  0.0318, -0.0309,  0.0345, -0.0341, -0.0078,\n",
      "        -0.0375, -0.0148], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0391, -0.0039,  0.0341,  0.0372, -0.0298,  0.0327, -0.0332, -0.0088,\n",
      "        -0.0371, -0.0174], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0362, -0.0026,  0.0320,  0.0370, -0.0285,  0.0315, -0.0319, -0.0097,\n",
      "        -0.0382, -0.0196], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0355, -0.0029,  0.0300,  0.0432, -0.0265,  0.0328, -0.0310, -0.0121,\n",
      "        -0.0382, -0.0169], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0362, -0.0038,  0.0286,  0.0461, -0.0315,  0.0320, -0.0356, -0.0123,\n",
      "        -0.0393, -0.0189], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0351, -0.0061,  0.0355,  0.0505, -0.0306,  0.0320, -0.0353, -0.0124,\n",
      "        -0.0334, -0.0158], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0348, -0.0091,  0.0324,  0.0504, -0.0295,  0.0345, -0.0326, -0.0129,\n",
      "        -0.0319, -0.0142], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0335, -0.0041,  0.0336,  0.0534, -0.0253,  0.0362, -0.0342, -0.0148,\n",
      "        -0.0308, -0.0104], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0304, -0.0046,  0.0327,  0.0520, -0.0193,  0.0409, -0.0364, -0.0149,\n",
      "        -0.0288, -0.0095], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0315, -0.0048,  0.0358,  0.0471, -0.0179,  0.0447, -0.0355, -0.0111,\n",
      "        -0.0358, -0.0115], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0323, -0.0035,  0.0380,  0.0472, -0.0144,  0.0390, -0.0366, -0.0153,\n",
      "        -0.0342, -0.0132], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0299, -0.0056,  0.0311,  0.0517, -0.0211,  0.0366, -0.0377, -0.0157,\n",
      "        -0.0320, -0.0141], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0287, -0.0053,  0.0333,  0.0515, -0.0231,  0.0377, -0.0387, -0.0197,\n",
      "        -0.0256, -0.0170], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0226, -0.0053,  0.0317,  0.0493, -0.0223,  0.0346, -0.0413, -0.0198,\n",
      "        -0.0244, -0.0194], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0209, -0.0050,  0.0379,  0.0507, -0.0268,  0.0413, -0.0391, -0.0203,\n",
      "        -0.0253, -0.0200], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0207,  0.0004,  0.0337,  0.0524, -0.0268,  0.0444, -0.0413, -0.0249,\n",
      "        -0.0242, -0.0205], grad_fn=<SliceBackward0>)\n",
      "ccra\n",
      "Theirs : tensor([ 0.0179, -0.0001,  0.0338,  0.0564, -0.0306,  0.0417, -0.0423, -0.0228,\n",
      "        -0.0229, -0.0182], grad_fn=<SliceBackward0>)\n",
      "Ours : [ 0.0345907  -0.01048923  0.01821981  0.02829448 -0.00866989  0.04091041\n",
      " -0.02458647  0.00242263 -0.03082916 -0.01788585]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03773357 -0.01054149  0.01944235  0.03443281 -0.00971653  0.04089953\n",
      " -0.02528114 -0.0004833  -0.03329596 -0.0196153 ]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03678795 -0.0122064   0.02081453  0.03479713 -0.00968709  0.0423816\n",
      " -0.02776056  0.00232834 -0.03162755 -0.01738064]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03602193 -0.00951464  0.01991252  0.03571185 -0.01203525  0.04176906\n",
      " -0.02988389  0.00330661 -0.02935955 -0.01741558]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03538687 -0.01102578  0.02147314  0.0376876  -0.01462834  0.04038863\n",
      " -0.02939173  0.00699263 -0.02920797 -0.01623017]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03500775 -0.01275734  0.02193138  0.04014757 -0.01427636  0.0404091\n",
      " -0.03038332  0.00649372 -0.02896333 -0.01569938]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03625117 -0.01530548  0.02155476  0.03781635 -0.01399703  0.04130697\n",
      " -0.03124673  0.00476155 -0.03161221 -0.016158  ]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03878628 -0.01408492  0.02102423  0.03344548 -0.01863361  0.04098471\n",
      " -0.03471593  0.00628498 -0.02847543 -0.01695551]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03838396 -0.01346277  0.01962486  0.03276421 -0.01927002  0.04681316\n",
      " -0.03212734  0.00618972 -0.03256902 -0.01657866]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03915668 -0.01352391  0.01855193  0.03233406 -0.01960248  0.04309946\n",
      " -0.03354216  0.00559547 -0.0334569  -0.01641133]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03747572 -0.01239792  0.02224325  0.03303165 -0.01946617  0.04499693\n",
      " -0.03366939  0.00204398 -0.03632152 -0.01875984]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03513801 -0.01087624  0.02023423  0.03201424 -0.02303546  0.04145304\n",
      " -0.03546448 -0.00195035 -0.0341298  -0.01923119]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03842385 -0.00526865  0.02271221  0.03461687 -0.02567548  0.04251432\n",
      " -0.03392308 -0.00514866 -0.03521595 -0.01913561]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03981663 -0.00508602  0.02498237  0.03700347 -0.02859968  0.04110347\n",
      " -0.03617512 -0.00465741 -0.0335061  -0.02086758]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.04358643 -0.00477833  0.02797058  0.03299068 -0.03005248  0.03751267\n",
      " -0.03726835 -0.00593931 -0.032799   -0.01719388]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.04070459 -0.00674216  0.03047109  0.03700941 -0.02710957  0.03603574\n",
      " -0.0360986  -0.0067482  -0.03299636 -0.01870249]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03851063 -0.00517466  0.02833293  0.03675393 -0.02581984  0.0349273\n",
      " -0.03590028 -0.0075535  -0.0337173  -0.02037031]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03817626 -0.00583921  0.02675721  0.0416261  -0.02185794  0.03280672\n",
      " -0.03527845 -0.0093186  -0.03392252 -0.02025444]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03817534 -0.00719137  0.02584109  0.04365782 -0.02635177  0.0335353\n",
      " -0.03848161 -0.0086379  -0.03110986 -0.0221945 ]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.0372239  -0.00960718  0.03187387  0.04692695 -0.02577672  0.03476185\n",
      " -0.04045    -0.00859938 -0.02607258 -0.0195093 ]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03639828 -0.01200981  0.02926531  0.04692053 -0.02522379  0.03691251\n",
      " -0.03821873 -0.00883425 -0.02520222 -0.01836751]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03589749 -0.0073962   0.03036961  0.05005413 -0.0214106   0.03918525\n",
      " -0.03893489 -0.01302561 -0.02472693 -0.01655726]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03342522 -0.00789916  0.03083786  0.05024191 -0.01653169  0.04252037\n",
      " -0.04043473 -0.01226152 -0.02314103 -0.01629649]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03406873 -0.00768898  0.03415645  0.04621409 -0.01529629  0.04538027\n",
      " -0.0401449  -0.00945564 -0.02980628 -0.01832211]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03598401 -0.00634347  0.03412399  0.04690005 -0.0117078   0.04090551\n",
      " -0.04070069 -0.01339461 -0.02901282 -0.01994034]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03387294 -0.00759736  0.02902935  0.05000214 -0.01730605  0.03929414\n",
      " -0.04150109 -0.0138097  -0.02597603 -0.02130068]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.03271186 -0.00628183  0.03141091  0.04957912 -0.01820636  0.03987891\n",
      " -0.03978956 -0.01721109 -0.02066679 -0.02369058]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.02725711 -0.00767496  0.03107435  0.04808921 -0.01848524  0.0372474\n",
      " -0.04009955 -0.01675146 -0.01994058 -0.02668688]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.02573043 -0.0074459   0.03635996  0.04865922 -0.02242917  0.0436125\n",
      " -0.03811243 -0.01793313 -0.02094074 -0.02798529]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.02552148 -0.0030365   0.03598517  0.04980904 -0.02262381  0.04637067\n",
      " -0.03989865 -0.02173317 -0.02022958 -0.02861182]\n",
      "SHAPE OF (1, 1082, 1024)\n",
      "Ours: (1, 1082, 1024)\n",
      "Ours : [ 0.02229092 -0.0021014   0.03513533  0.05448132 -0.0241203   0.04485446\n",
      " -0.04186331 -0.01903016 -0.01996266 -0.02751537]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 1107846 / 1107968 (100.0%)\nGreatest absolute difference: 0.6334989666938782 at index (0, 95, 23) (up to 1e-05 allowed)\nGreatest relative difference: 3572257.75 at index (0, 508, 407) (up to 1.3e-06 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m our_y \u001b[38;5;241m=\u001b[39m our_gpt(our_x)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(their_y[0][0, 0, :10])\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print(our_y[0, 0, :10])\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheir_y\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mour_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m our_gpt\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/torch/testing/_comparison.py:1530\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1508\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1509\u001b[0m     actual,\n\u001b[1;32m   1510\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1526\u001b[0m )\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 1107846 / 1107968 (100.0%)\nGreatest absolute difference: 0.6334989666938782 at index (0, 95, 23) (up to 1e-05 allowed)\nGreatest relative difference: 3572257.75 at index (0, 508, 407) (up to 1.3e-06 allowed)"
     ]
    }
   ],
   "source": [
    "our_gpt = TransformerLayer(their_config, jax.random.PRNGKey(1))\n",
    "\n",
    "\n",
    "# Function to update the JAX model parameters\n",
    "def update_params(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    # for jax_key, to\\rch_key in torch_to_jax_keys:\n",
    "    if path in torch_params.keys():\n",
    "        # if \"ln_\" in path:\n",
    "        #     print(x.shape)\n",
    "        #     print(torch_params[path].shape)\n",
    "        #     print(path)\n",
    "        #     return x\n",
    "        # if \"ln_\" in path:\n",
    "        #     print(path)\n",
    "        return jax.numpy.array(torch_params[path])\n",
    "    # print(path)\n",
    "    return x\n",
    "\n",
    "\n",
    "# our_gpt = jax.tree_util.tree_map_with_path(update_params, our_gpt)\n",
    "our_gpt = jax.tree_util.tree_map_with_path(update_params, our_gpt)\n",
    "# # our_gpt.h.0.ln_1.\n",
    "eqx.tree_serialise_leaves(\"xttsgpt.eqx\", our_gpt)\n",
    "our_gpt = eqx.tree_deserialise_leaves(\"xttsgpt.eqx\", our_gpt)\n",
    "\n",
    "our_x = jax.random.randint(jax.random.PRNGKey(2), shape=(1, 1082), minval=0, maxval=255)\n",
    "their_x = torch.from_numpy(numpy.array(our_x))\n",
    "print(our_x[0, 0])\n",
    "print(their_x[0, 0])\n",
    "\n",
    "their_y = their_gpt(their_x)\n",
    "# (10, 16, 1082, 64) is the shape of the attentions\n",
    "our_y = our_gpt(our_x)\n",
    "\n",
    "# print(their_y[0][0, 0, :10])\n",
    "# print(our_y[0, 0, :10])\n",
    "torch.testing.assert_close(their_y[0], torch.from_numpy(numpy.array(our_y)))\n",
    "del our_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_x = jax.random.normal(\n",
    "    jax.random.PRNGKey(2),\n",
    "    shape=(1, 64, 1024),\n",
    "    dtype=numpy.float32,\n",
    ")\n",
    "their_x = torch.from_numpy(numpy.array(our_x))\n",
    "\n",
    "their_y = their_gpt.h[0].ln_1(their_x)\n",
    "# (10, 16, 1082, 64) is the shape of the attentions\n",
    "our_y = jax.vmap(jax.vmap(our_gpt.h[0].ln_1))(our_x)\n",
    "\n",
    "# print(their_y[0][0, 0, :10])\n",
    "# print(our_y[0, 0, :10])\n",
    "torch.testing.assert_close(their_y, torch.from_numpy(numpy.array(our_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class GPT(eqx.Module):\n",
    "    transformer: TransformerLayer\n",
    "    lm_head: nn.Linear\n",
    "\n",
    "    def __init__(self, config, key):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "\n",
    "        self.transformer = TransformerLayer(config, key1)\n",
    "        self.lm_head = nn.Linear(\n",
    "            config.n_embd, config.vocab_size, use_bias=False, key=key2\n",
    "        )\n",
    "\n",
    "    def __call__(self, token_ids):\n",
    "        y = self.transformer(token_ids)\n",
    "        logits = jax.vmap(self.lm_head)(y)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our method with the one implemented in nanoJAXGPT:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
